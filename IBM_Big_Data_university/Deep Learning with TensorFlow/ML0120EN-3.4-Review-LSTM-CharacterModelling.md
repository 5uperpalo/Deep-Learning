
<a href="https://www.bigdatauniversity.com"><img src = "https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png" width = 300, align = "center"></a>


# <center> Text generation using RNN/LSTM (Character-level)</center>

<div class="alert alert-block alert-info">
<font size = 3><strong>In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network</strong></font>
<br>    
- <a href="#intro">Introduction</a>
<br>
- <p><a href="#arch">Architectures</a></p>
    - <a href="#lstm">Long Short-Term Memory Model (LSTM)</a>

- <p><a href="#build">Building a LSTM with TensorFlow</a></p>
</div>
----------------

This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  
The RNN can then be used to generate text character by character that will look like the original training data. 

This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).




First, import the requiered libraries:


```python
import tensorflow as tf
import time
import codecs
import os
import collections
from six.moves import cPickle
import numpy as np
#from tensorflow.python.ops import rnn_cell
#from tensorflow.python.ops import seq2seq
```

### Data loader
The following cell is a class that help to read data from input file.


```python
class TextLoader():
    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.encoding = encoding

        input_file = os.path.join(data_dir, "input.txt")
        vocab_file = os.path.join(data_dir, "vocab.pkl")
        tensor_file = os.path.join(data_dir, "data.npy")

        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):
            print("reading text file")
            self.preprocess(input_file, vocab_file, tensor_file)
        else:
            print("loading preprocessed files")
            self.load_preprocessed(vocab_file, tensor_file)
        self.create_batches()
        self.reset_batch_pointer()

    def preprocess(self, input_file, vocab_file, tensor_file):
        with codecs.open(input_file, "r", encoding=self.encoding) as f:
            data = f.read()
        counter = collections.Counter(data)
        count_pairs = sorted(counter.items(), key=lambda x: -x[1])
        self.chars, _ = zip(*count_pairs)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        with open(vocab_file, 'wb') as f:
            cPickle.dump(self.chars, f)
        self.tensor = np.array(list(map(self.vocab.get, data)))
        np.save(tensor_file, self.tensor)

    def load_preprocessed(self, vocab_file, tensor_file):
        with open(vocab_file, 'rb') as f:
            self.chars = cPickle.load(f)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        self.tensor = np.load(tensor_file)
        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))

    def create_batches(self):
        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))

        # When the data (tensor) is too small, let's give them a better error message
        if self.num_batches==0:
            assert False, "Not enough data. Make seq_length and batch_size small."

        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]
        xdata = self.tensor
        ydata = np.copy(self.tensor)
        ydata[:-1] = xdata[1:]
        ydata[-1] = xdata[0]
        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)
        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)


    def next_batch(self):
        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]
        self.pointer += 1
        return x, y

    def reset_batch_pointer(self):
        self.pointer = 0
```

### Parameters
#### Batch, number_of_batch, batch_size and seq_length
what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  

Lets assume the input is this sentence: '__here is an example__'. Then:
- txt_length = 18  
- seq_length = 3  
- batch_size = 2  
- number_of_batchs = 18/3*2 = 3
- batch = array (['h','e','r'],['e',' ','i'])
- sample Seq = 'her'  

Ok, now, lets look at a real dataset, with real parameters. 


```python
seq_length = 50 # RNN sequence length
batch_size = 60  # minibatch size, i.e. size of data in each epoch
num_epochs = 125 # you should change it to 50 if you want to see a relatively good results
learning_rate = 0.002
decay_rate = 0.97
rnn_size = 128 # size of RNN hidden state (output dimension)
num_layers = 2 #number of layers in the RNN
```

We download the input file, and print a part of it:


```python
!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt 
with open('input.txt', 'r') as f:
    read_data = f.read()
    print read_data[0:100]
f.closed
```

    2018-02-28 16:57:13 URL:https://public.boxcloud.com/d/1/NJq-AwAm_r_q7wzdQQNQSCq_upwykX63huF3CmPyFY12BkXibEGjA0uAyuLNcOQqlo6LAaYHI81TDU50IQ7mBrTiZvwGIvur0lmWCX_sf_Wo4l3EuwcEMKyHKLvAIDAdY4ArBksWr_k8UNDlO3H68Cl-9iADo1zv7AHE2_oecsTMw43flMwDr99VqNl7iY5fDb2etyG7OLQxEF73j0S3AWkWoDHZjUqsn6S2tiaur_2oDI2V9FtpR0N78OJmndRvBbw3JWv-MI6aVxQYBbAYzUmczaakFX4RxllkYTr9JZglGZ2gtIFOdGBvkGlbHqioNb5qW8LK4-QA00ppc7JaV5f5REKJzamUYlPrXO8KBMuV17TREhiAa5jqVGai_UOXjl-qXEoyBcQhsE300CG3bgWTF4nDvO-8fAf8en2pWUGecH8vwI9t6KAiucauYgtlkQ-k-y5PZniqLhmNWJ6PQoIVavC2Hd7WE1UGfUQ10mPGb6XisD6w5Ko0JiGy__WDlGb95xXoncLHoqfdxYzUNP6BkU0kTyEukSV1Gvs_YzEZUXutrAVgFQUKrW1EdWUj21AVOScfFgduMzw-REKCYKqa9tyv030ax_jUr2kLuOc1a_BivbDLOQxd5GGi5hyCmNibhRj37s8MEgCBez6QK5haYVutvCG3F99EHwX7CZwQMRzwXvcxgcHNuP6EAKWJnUfxTOB22uSw5be1pT2mHhWTfkmjAkccxwG3Tr4P42DTANOiZ0RTL_3GFrnOj6-ghCQCFUTzr5t9r1Xa4Ku6Sc2ON9XLeXS913zjN5gYwuUDHhIYtC4sBdBj9MeO-9IeCOutiEWqa5wp9XIwbWKH2JLtbdttzIt9_4CPAt1HkNBcHwhVTXLM1EiNhKXkxXgkYXoZ8vZzCcrRCgzn8ilECKU7Qx9c0vJv4IgsM9PdFNwoBZllIxiBFLTqV8s5SDnZfDFMtfP11ja0bVg1xPMx11Vq4ayy399d2y42ku1uEnwXncrAQ7K2BxuOD2qBaEMBXOWxpPVyGh_HFW8BJecttZ-PjjpYdRbSoKBjaJK5YcYhz-gRc6cqYCLxHoqwry3e8iAAYywNp7zBjyyfBZAy-Do3tC4PtB2FSGw9iGeJNt7NXhv2nSTS-Y3gkN_NlLmvloEXJVxEFwisaMRk5O94xjKXkKCEqBUgeXQ8sXNXlkyGGdFO2bPi28zKFuv4V2wZiqA4Ean8Zv6qxAuOJzJ07YPJ0GFDsfXFy8q0QX4R_hX5MngrHr_A9bBnwuMcL0tDiljovLXcCSAcDtls3t_frxrdQ0mso2na0hA4BiamGkKwPbERrnmdRtkOUNcC_0efvo7raAzHk3Yqgij6DArizw../download [1115393/1115393] -> "input.txt" [1]
    First Citizen:
    Before we proceed any further, hear me speak.
    
    All:
    Speak, speak.
    
    First Citizen:
    You





    True



Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:


```python
data_loader = TextLoader('', batch_size, seq_length)
vocab_size = data_loader.vocab_size
print "vocabulary size:" ,data_loader.vocab_size
print "Characters:" ,data_loader.chars
print "vocab number of 'F':",data_loader.vocab['F']
print "Character sequences (first batch):", data_loader.x_batches[0]
```

    reading text file
    vocabulary size: 65
    Characters: (u' ', u'e', u't', u'o', u'a', u'h', u's', u'r', u'n', u'i', u'\n', u'l', u'd', u'u', u'm', u'y', u',', u'w', u'f', u'c', u'g', u'I', u'b', u'p', u':', u'.', u'A', u'v', u'k', u'T', u"'", u'E', u'O', u'N', u'R', u'S', u'L', u'C', u';', u'W', u'U', u'H', u'M', u'B', u'?', u'G', u'!', u'D', u'-', u'F', u'Y', u'P', u'K', u'V', u'j', u'q', u'x', u'z', u'J', u'Q', u'Z', u'X', u'3', u'&', u'$')
    vocab number of 'F': 49
    Character sequences (first batch): [[49  9  7 ...,  1  4  7]
     [19  4 14 ..., 14  9 20]
     [ 8 20 10 ...,  8 10 18]
     ..., 
     [21  2  0 ...,  0 21  0]
     [ 9  7  7 ...,  0  2  3]
     [ 3  7  0 ...,  5  9 23]]


### Input and output


```python
x,y = data_loader.next_batch()
x
```




    array([[49,  9,  7, ...,  1,  4,  7],
           [19,  4, 14, ..., 14,  9, 20],
           [ 8, 20, 10, ...,  8, 10, 18],
           ..., 
           [21,  2,  0, ...,  0, 21,  0],
           [ 9,  7,  7, ...,  0,  2,  3],
           [ 3,  7,  0, ...,  5,  9, 23]])




```python
x.shape  #batch_size =60, seq_length=50
```




    (60, 50)



Here, __y__ is the next character for each character in __x__:


```python
y
```




    array([[ 9,  7,  6, ...,  4,  7,  0],
           [ 4, 14, 22, ...,  9, 20,  5],
           [20, 10, 29, ..., 10, 18,  4],
           ..., 
           [ 2,  0,  6, ..., 21,  0,  6],
           [ 7,  7,  4, ...,  2,  3,  0],
           [ 7,  0, 33, ...,  9, 23,  0]])



### LSTM Architecture
Each LSTM cell has 5 parts:
1. Input
2. prv_state
3. prv_output
4. new_state
5. new_output


- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.
- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size
- An LSTM keeps two pieces of information as it propagates through time: 
    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. "num_units" is equivalant to "size of RNN hidden state". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.
    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. 


#### num_layers = 2 
- number of layers in the RNN, is defined by num_layers
- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order.

### Defining stacked RNN Cell

__BasicRNNCell__ is the most basic RNN cell.


```python
cell = tf.contrib.rnn.BasicRNNCell(rnn_size)
```

    Couldn't import dot_parser, loading of dot files will not be possible.



```python
# a two layer cell
stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)
```


```python
# hidden state size
stacked_cell.output_size
```




    128



__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:


```python
stacked_cell.state_size
```




    (128, 128)



Lets define input data:


```python
input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50
input_data
```




    <tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>



and target data:


```python
targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50
targets
```




    <tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>



The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.

__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size
representing the batch size.


```python
initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128
```

Lets check the value of the input_data again:


```python
session = tf.Session()
feed_dict={input_data:x, targets:y}
session.run(input_data, feed_dict)
```




    array([[49,  9,  7, ...,  1,  4,  7],
           [19,  4, 14, ..., 14,  9, 20],
           [ 8, 20, 10, ...,  8, 10, 18],
           ..., 
           [21,  2,  0, ...,  0, 21,  0],
           [ 9,  7,  7, ...,  0,  2,  3],
           [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)



### Embedding
In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix.

__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. 


```python
with tf.variable_scope('rnnlm', reuse=False):
    softmax_w = tf.get_variable("softmax_w", [rnn_size, vocab_size]) #128x65
    softmax_b = tf.get_variable("softmax_b", [vocab_size]) # 1x65)
    #with tf.device("/cpu:0"):
        
    # embedding variable is initialized randomely
    embedding = tf.get_variable("embedding", [vocab_size, rnn_size])  #65x128

    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding
    # it creates a 60*50*[1*128] matrix
    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character
    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]
    # split: Splits a tensor into sub tensors.
    # syntax:  tf.split(split_dim, num_split, value, name='split')
    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]
    inputs = tf.split(em, seq_length, 1)
    # It will convert the list to 50 matrix of [60x128]
    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]
```

Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:

Embedding variable is initialized with random values:


```python
session.run(tf.global_variables_initializer())
#print embedding.shape
session.run(embedding)
```




    array([[-0.07650967, -0.12438035,  0.09362756, ...,  0.08350749,
            -0.08659657,  0.17252378],
           [-0.00598076,  0.02082266,  0.16045846, ...,  0.10890745,
            -0.06203505, -0.03711136],
           [-0.06277651, -0.13724492, -0.06775628, ..., -0.13276184,
             0.16389634,  0.05889948],
           ..., 
           [-0.12915586,  0.00097284, -0.08840809, ...,  0.01574053,
             0.00188626, -0.05425745],
           [-0.14020562, -0.06312585, -0.14362997, ...,  0.14210467,
             0.17116739,  0.07351638],
           [ 0.12499247,  0.07665448,  0.04871543, ...,  0.10589011,
             0.16359468,  0.10370238]], dtype=float32)



The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character


```python
em = tf.nn.embedding_lookup(embedding, input_data)
emp = session.run(em,feed_dict={input_data:x})
print emp.shape
emp[0]
```

    (60, 50, 128)





    array([[ 0.03999361, -0.12599   , -0.01121725, ...,  0.13509642,
             0.00119613,  0.09117402],
           [ 0.16091307,  0.09199663,  0.16070868, ...,  0.04406957,
            -0.03514118, -0.048729  ],
           [-0.01720591, -0.1220455 , -0.07514866, ..., -0.17485715,
            -0.13811716,  0.03457347],
           ..., 
           [-0.00598076,  0.02082266,  0.16045846, ...,  0.10890745,
            -0.06203505, -0.03711136],
           [ 0.01995617,  0.04702556, -0.02302685, ...,  0.05669399,
            -0.13343254,  0.15512829],
           [-0.01720591, -0.1220455 , -0.07514866, ..., -0.17485715,
            -0.13811716,  0.03457347]], dtype=float32)



Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences.


```python
inputs = tf.split(em, seq_length, 1)
inputs = [tf.squeeze(input_, [1]) for input_ in inputs]
inputs[0:5]
```




    [<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]



### Feeding a batch of 50 sequence to a RNN:

The feeding process for iputs is as following:

- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  
- Step 2:  second character of each of the 50 sentences is input in parallel. 
- Step n: nth character of each of the 50 sentences is input in parallel.  

The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. 


```python
session.run(inputs[0],feed_dict={input_data:x})
```




    array([[ 0.03999361, -0.12599   , -0.01121725, ...,  0.13509642,
             0.00119613,  0.09117402],
           [-0.09763654, -0.14824484,  0.13581119, ..., -0.13718082,
            -0.07377374,  0.0652775 ],
           [ 0.04011783,  0.10713361, -0.15824106, ...,  0.11261217,
             0.17442472,  0.17363347],
           ..., 
           [ 0.03510725,  0.02459662, -0.09307976, ...,  0.16007866,
             0.13584878, -0.158923  ],
           [ 0.16091307,  0.09199663,  0.16070868, ...,  0.04406957,
            -0.03514118, -0.048729  ],
           [-0.02101439,  0.10949008,  0.04095122, ...,  0.14226075,
            -0.02560914, -0.0174005 ]], dtype=float32)



Feeding the RNN with one batch, we can check the new output and new state of network:


```python
#outputs is 50x[60*128]
outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')
new_state
```




    (<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)




```python
outputs[0:5]
```




    [<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]



Let's check the output of network after feeding it with first batch:


```python
first_output = outputs[0]
session.run(tf.global_variables_initializer())
session.run(first_output,feed_dict={input_data:x})
```




    array([[-0.01161624, -0.06864932, -0.0046166 , ..., -0.04588962,
            -0.05332525,  0.00664235],
           [-0.01779474,  0.10624772, -0.01542302, ...,  0.04820808,
            -0.00615254, -0.05267301],
           [-0.0024244 , -0.0406689 ,  0.02272658, ..., -0.0684206 ,
             0.17272785, -0.13608235],
           ..., 
           [ 0.03895387, -0.00979013,  0.03628931, ..., -0.10128211,
             0.05430704,  0.0241865 ],
           [ 0.00422184,  0.01741251, -0.04162872, ...,  0.01679415,
             0.02598278,  0.04821396],
           [-0.01006369,  0.1145976 ,  0.02258371, ...,  0.0051184 ,
             0.06707979, -0.04942993]], dtype=float32)



As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:

softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]

We can do it step-by-step:


```python
output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])
output
```




    <tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>




```python
logits = tf.matmul(output, softmax_w) + softmax_b
logits
```




    <tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>




```python
probs = tf.nn.softmax(logits)
probs
```




    <tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>



Here is the probablity of the next chracter in all batches:


```python
session.run(tf.global_variables_initializer())
session.run(probs,feed_dict={input_data:x})
```




    array([[ 0.01653045,  0.01197249,  0.01737318, ...,  0.01280643,
             0.01353179,  0.01484838],
           [ 0.01629052,  0.01049038,  0.01647009, ...,  0.01104559,
             0.0139896 ,  0.01504616],
           [ 0.01987128,  0.01187824,  0.01436006, ...,  0.00958281,
             0.01402934,  0.01505971],
           ..., 
           [ 0.02101325,  0.01458336,  0.01708128, ...,  0.01239767,
             0.01180489,  0.01089225],
           [ 0.0125778 ,  0.01066843,  0.02193921, ...,  0.01090299,
             0.01210158,  0.01370777],
           [ 0.01471571,  0.01286325,  0.01920282, ...,  0.01163267,
             0.01381399,  0.01650019]], dtype=float32)



Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?


```python
grad_clip =5.
tvars = tf.trainable_variables()
tvars
```




    [<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,
     <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,
     <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,
     <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,
     <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]



# All together
Now, let's put all of parts together in a class, and train the model:


```python
class LSTMModel():
    def __init__(self,sample=False):
        rnn_size = 128 # size of RNN hidden state vector
        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch
        seq_length = 50 # RNN sequence length
        num_layers = 2 # number of layers in the RNN
        vocab_size = 65
        grad_clip = 5.
        if sample:
            print(">> sample mode:")
            batch_size = 1
            seq_length = 1
        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. 
        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)
        # model.cell.state_size is (128, 128)
        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)

        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name="input_data")
        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name="targets")
        # Initial state of the LSTM memory.
        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. 
        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size

        with tf.variable_scope('rnnlm_class1'):
            softmax_w = tf.get_variable("softmax_w", [rnn_size, vocab_size]) #128x65
            softmax_b = tf.get_variable("softmax_b", [vocab_size]) # 1x65
            with tf.device("/cpu:0"):
                embedding = tf.get_variable("embedding", [vocab_size, rnn_size])  #65x128
                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)
                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]
                #inputs = tf.split(em, seq_length, 1)
                
                


        # The value of state is updated after processing each batch of chars.
        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')
        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])
        self.logits = tf.matmul(output, softmax_w) + softmax_b
        self.probs = tf.nn.softmax(self.logits)
        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],
                [tf.reshape(self.targets, [-1])],
                [tf.ones([batch_size * seq_length])],
                vocab_size)
        self.cost = tf.reduce_sum(loss) / batch_size / seq_length
        self.final_state = last_state
        self.lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)
        optimizer = tf.train.AdamOptimizer(self.lr)
        self.train_op = optimizer.apply_gradients(zip(grads, tvars))
    
    
    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):
        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))
        #print state
        for char in prime[:-1]:
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state:state}
            [state] = sess.run([self.final_state], feed)

        def weighted_pick(weights):
            t = np.cumsum(weights)
            s = np.sum(weights)
            return(int(np.searchsorted(t, np.random.rand(1)*s)))

        ret = prime
        char = prime[-1]
        for n in range(num):
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state:state}
            [probs, state] = sess.run([self.probs, self.final_state], feed)
            p = probs[0]

            if sampling_type == 0:
                sample = np.argmax(p)
            elif sampling_type == 2:
                if char == ' ':
                    sample = weighted_pick(p)
                else:
                    sample = np.argmax(p)
            else: # sampling_type == 1 default:
                sample = weighted_pick(p)

            pred = chars[sample]
            ret += pred
            char = pred
        return ret
```

### Creating the LSTM object
Now we create a LSTM model:


```python
with tf.variable_scope("rnn"):
    model = LSTMModel()
```

# Train usinng LSTMModel class
We can train our model through feeding batches:


```python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher
        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))
        data_loader.reset_batch_pointer()
        state = sess.run(model.initial_state) # (2x[60x128])
        for b in range(data_loader.num_batches): #for each batch
            start = time.time()
            x, y = data_loader.next_batch()
            feed = {model.input_data: x, model.targets: y, model.initial_state:state}
            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)
            end = time.time()
        print("{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}" \
                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))
        with tf.variable_scope("rnn", reuse=True):
            sample_model = LSTMModel(sample=True)
            print sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=50, prime='The ', sampling_type=1)
            print '----------------------------------'
```

    370/46375 (epoch 0), train_loss = 1.936, time/batch = 0.068
    >> sample mode:
    The shis weret the spation me: hing stoo the saces I
    c
    ----------------------------------
    741/46375 (epoch 1), train_loss = 1.775, time/batch = 0.093
    >> sample mode:
    The for here your baith helton at deatherd, I sold.
    
    F
    ----------------------------------
    1112/46375 (epoch 2), train_loss = 1.696, time/batch = 0.085
    >> sample mode:
    The sugelon:
    This for this can gains you sould if
    Vemp
    ----------------------------------
    1483/46375 (epoch 3), train_loss = 1.644, time/batch = 0.101
    >> sample mode:
    The mack against beout?
    
    KING RICHARD III:
    Show in the
    ----------------------------------
    1854/46375 (epoch 4), train_loss = 1.609, time/batch = 0.068
    >> sample mode:
    The thy wind,
    I should parlike a bed,
    I was destry;
    Bs
    ----------------------------------
    2225/46375 (epoch 5), train_loss = 1.586, time/batch = 0.096
    >> sample mode:
    The Froth, thou well; he with every makes order to sub
    ----------------------------------
    2596/46375 (epoch 6), train_loss = 1.570, time/batch = 0.082
    >> sample mode:
    The did, for the heart-to like to humble we passined t
    ----------------------------------
    2967/46375 (epoch 7), train_loss = 1.558, time/batch = 0.083
    >> sample mode:
    The madeps acqualiness!
    Dull I must he death;
    From res
    ----------------------------------
    3338/46375 (epoch 8), train_loss = 1.548, time/batch = 0.079
    >> sample mode:
    The if inquire, unking my prevenge against the nightly
    ----------------------------------
    3709/46375 (epoch 9), train_loss = 1.539, time/batch = 0.067
    >> sample mode:
    The sir, bater be
    it:
    Thy, only; whan thee green
    but d
    ----------------------------------
    4080/46375 (epoch 10), train_loss = 1.532, time/batch = 0.070
    >> sample mode:
    The hope this nonought out with a money would,
    Not sce
    ----------------------------------
    4451/46375 (epoch 11), train_loss = 1.526, time/batch = 0.090
    >> sample mode:
    The dull--
    Your lead
    She that myselike I know, I shall
    ----------------------------------
    4822/46375 (epoch 12), train_loss = 1.520, time/batch = 0.074
    >> sample mode:
    The betrience I confess'd as turn:
    Some knowings Loldi
    ----------------------------------
    5193/46375 (epoch 13), train_loss = 1.515, time/batch = 0.070
    >> sample mode:
    The fearful tell the bendop Richmant's nothing, what n
    ----------------------------------
    5564/46375 (epoch 14), train_loss = 1.510, time/batch = 0.064
    >> sample mode:
    The is't.
    
    PETER:
    Rescurbligns.
    
    CURTIS:
    Shell, stract
    ----------------------------------
    5935/46375 (epoch 15), train_loss = 1.506, time/batch = 0.069
    >> sample mode:
    The will to come.
    
    CAMILLO:
    Salway,
    Or dow medune.
    
    DU
    ----------------------------------
    6306/46375 (epoch 16), train_loss = 1.502, time/batch = 0.085
    >> sample mode:
    The I bay faupoat a deturns: sir, be
    I mean two of the
    ----------------------------------
    6677/46375 (epoch 17), train_loss = 1.498, time/batch = 0.078
    >> sample mode:
    The is't! Sister undon her.
    
    GRUKI:
    Whomle stradly tro
    ----------------------------------
    7048/46375 (epoch 18), train_loss = 1.495, time/batch = 0.067
    >> sample mode:
    The Thou speak high! Monter our law in what fellow the
    ----------------------------------
    7419/46375 (epoch 19), train_loss = 1.492, time/batch = 0.066
    >> sample mode:
    The reason,, my married an,
    He is noting thy father is
    ----------------------------------
    7790/46375 (epoch 20), train_loss = 1.490, time/batch = 0.074
    >> sample mode:
    The visiting kneel which what whose swove tills die mu
    ----------------------------------
    8161/46375 (epoch 21), train_loss = 1.487, time/batch = 0.066
    >> sample mode:
    The dike, Ave more.
    
    TURBISSARD:
    A descale I avoil jus
    ----------------------------------
    8532/46375 (epoch 22), train_loss = 1.485, time/batch = 0.104
    >> sample mode:
    The sort, how ever a point these buried it;
    Whence lik
    ----------------------------------
    8903/46375 (epoch 23), train_loss = 1.482, time/batch = 0.083
    >> sample mode:
    The of straight my octake same a mother,--I am
    very ge
    ----------------------------------
    9274/46375 (epoch 24), train_loss = 1.480, time/batch = 0.068
    >> sample mode:
    The your in--
    
    Nurse:
    No, my lord, must neecles it liv
    ----------------------------------
    9645/46375 (epoch 25), train_loss = 1.478, time/batch = 0.077
    >> sample mode:
    The she in ghassion evouge,
    We knowledge and hungly, a
    ----------------------------------
    10016/46375 (epoch 26), train_loss = 1.476, time/batch = 0.078
    >> sample mode:
    The Ditule; sir: down: that every most to the fingers 
    ----------------------------------
    10387/46375 (epoch 27), train_loss = 1.474, time/batch = 0.074
    >> sample mode:
    The dien, where, sir, will I grscale as I was it is: a
    ----------------------------------
    10758/46375 (epoch 28), train_loss = 1.471, time/batch = 0.071
    >> sample mode:
    The purches, and there impession: by lords; if thou st
    ----------------------------------
    11129/46375 (epoch 29), train_loss = 1.469, time/batch = 0.081
    >> sample mode:
    The she officut;
    And was not people for the cater coun
    ----------------------------------
    11500/46375 (epoch 30), train_loss = 1.467, time/batch = 0.062
    >> sample mode:
    The viamentio an arming them.
    
    ESCALUS:
    Frcation!
    
    DER
    ----------------------------------
    11871/46375 (epoch 31), train_loss = 1.466, time/batch = 0.069
    >> sample mode:
    The I, I
    me prizaiding unbid,
    By blas.
    
    GREGORY;
    Of hi
    ----------------------------------
    12242/46375 (epoch 32), train_loss = 1.464, time/batch = 0.067
    >> sample mode:
    The shall be live.
    
    PETRUCHIO:
    And your lady!
    
    GLOUCES
    ----------------------------------
    12613/46375 (epoch 33), train_loss = 1.462, time/batch = 0.073
    >> sample mode:
    The is villain;
    Whom, in him
    Who send doth love.
    It is
    ----------------------------------
    12984/46375 (epoch 34), train_loss = 1.461, time/batch = 0.063
    >> sample mode:
    The issue against me mees botter make a lips his her,
    
    ----------------------------------
    13355/46375 (epoch 35), train_loss = 1.459, time/batch = 0.093
    >> sample mode:
    The Voling me,
    A?
    
    SBOLONGA:
    Give As slise,--O but gon
    ----------------------------------
    13726/46375 (epoch 36), train_loss = 1.458, time/batch = 0.082
    >> sample mode:
    The shall speak.
    
    JULIET:
    O can share
    Procene a inhere
    ----------------------------------
    14097/46375 (epoch 37), train_loss = 1.457, time/batch = 0.076
    >> sample mode:
    The sungin to That cause. Baptyelt altor man shall be 
    ----------------------------------
    14468/46375 (epoch 38), train_loss = 1.456, time/batch = 0.084
    >> sample mode:
    The follteds.
    
    DUCHESS OF YORK:
    Why, sweet frown, let 
    ----------------------------------
    14839/46375 (epoch 39), train_loss = 1.455, time/batch = 0.088
    >> sample mode:
    The is uncimedful pariage,
    As I do latest't,
    Is conthe
    ----------------------------------
    15210/46375 (epoch 40), train_loss = 1.454, time/batch = 0.096
    >> sample mode:
    The I'll praction!
    Fast.
    
    First Citizen:
    Villame in al
    ----------------------------------
    15581/46375 (epoch 41), train_loss = 1.453, time/batch = 0.077
    >> sample mode:
    The like thus, whose son,
    Upon his precuce
    And come to
    ----------------------------------
    15952/46375 (epoch 42), train_loss = 1.452, time/batch = 0.069
    >> sample mode:
    The Froth, here that threar things his no
    forth me,
    Th
    ----------------------------------
    16323/46375 (epoch 43), train_loss = 1.452, time/batch = 0.069
    >> sample mode:
    The kiss you.
    Ah, whom no losing.
    
    TCANOS:
    Cousin off 
    ----------------------------------
    16694/46375 (epoch 44), train_loss = 1.451, time/batch = 0.074
    >> sample mode:
    The shall we in York, fool to-suits quee?
    
    AUTOLYCUS:
    
    ----------------------------------
    17065/46375 (epoch 45), train_loss = 1.450, time/batch = 0.062
    >> sample mode:
    The varse;
    I'll though I fined man: I will fie, maticj
    ----------------------------------
    17436/46375 (epoch 46), train_loss = 1.449, time/batch = 0.063
    >> sample mode:
    The subs,'-let With one wounded foul such,
    And hungred
    ----------------------------------
    17807/46375 (epoch 47), train_loss = 1.449, time/batch = 0.069
    >> sample mode:
    The shall sovereignt.
    For that any upon his sea.
    Now y
    ----------------------------------
    18178/46375 (epoch 48), train_loss = 1.448, time/batch = 0.059
    >> sample mode:
    The should let's Lord Nie flothes myself--
    
    RICHARD:
    S
    ----------------------------------
    18549/46375 (epoch 49), train_loss = 1.448, time/batch = 0.064
    >> sample mode:
    The now weer her; one even peace?
    
    TRANIO:
    I must dost
    ----------------------------------
    18920/46375 (epoch 50), train_loss = 1.447, time/batch = 0.070
    >> sample mode:
    The were a concled our apwing of dietre and true one t
    ----------------------------------
    19291/46375 (epoch 51), train_loss = 1.446, time/batch = 0.075
    >> sample mode:
    The you would not bewral fought arrived: I not
    The cha
    ----------------------------------
    19662/46375 (epoch 52), train_loss = 1.446, time/batch = 0.088
    >> sample mode:
    The she
    you with your place, the do you, sir.
    
    MENENIU
    ----------------------------------
    20033/46375 (epoch 53), train_loss = 1.445, time/batch = 0.098
    >> sample mode:
    The shall not
    That that send upon his brothers, with t
    ----------------------------------
    20404/46375 (epoch 54), train_loss = 1.445, time/batch = 0.075
    >> sample mode:
    The disops my hunging the critful bonding.
    
    PETRUCHIO:
    ----------------------------------
    20775/46375 (epoch 55), train_loss = 1.444, time/batch = 0.082
    >> sample mode:
    The shrobbsty you subject, my hand.
    
    ANGTHAN:
    What hou
    ----------------------------------
    21146/46375 (epoch 56), train_loss = 1.443, time/batch = 0.062
    >> sample mode:
    The Guut in gall-I swear,
    That it well, by of play to 
    ----------------------------------
    21517/46375 (epoch 57), train_loss = 1.443, time/batch = 0.096
    >> sample mode:
    The shall tell God you, pardon, your love to date for 
    ----------------------------------
    21888/46375 (epoch 58), train_loss = 1.442, time/batch = 0.088
    >> sample mode:
    The death,
    The roer and well.
    
    CORIOLANUS:
    Well, so; h
    ----------------------------------
    22259/46375 (epoch 59), train_loss = 1.442, time/batch = 0.091
    >> sample mode:
    The Shore. And if so ain
    Dires, would broothe, ridest 
    ----------------------------------
    22630/46375 (epoch 60), train_loss = 1.441, time/batch = 0.070
    >> sample mode:
    The lords;
    Fie, away, indeed
    That now, are which is a 
    ----------------------------------
    23001/46375 (epoch 61), train_loss = 1.441, time/batch = 0.069
    >> sample mode:
    The is know both!
    Fire-calse: O'll be gone.
    
    ELBOW:
    Ho
    ----------------------------------
    23372/46375 (epoch 62), train_loss = 1.440, time/batch = 0.085
    >> sample mode:
    The shall live
    And witheriful
    Will I knew them the for
    ----------------------------------
    23743/46375 (epoch 63), train_loss = 1.440, time/batch = 0.065
    >> sample mode:
    The shall
    prupice,
    In have knees, follow'd all be prin
    ----------------------------------
    24114/46375 (epoch 64), train_loss = 1.439, time/batch = 0.081
    >> sample mode:
    The is manisk your time in the patie,
    Or foat and says
    ----------------------------------
    24485/46375 (epoch 65), train_loss = 1.439, time/batch = 0.077
    >> sample mode:
    The shall be true his ables another weary
    Than what am
    ----------------------------------
    24856/46375 (epoch 66), train_loss = 1.439, time/batch = 0.081
    >> sample mode:
    The shall I marry his head;
    Not paintifice thee draw, 
    ----------------------------------
    25227/46375 (epoch 67), train_loss = 1.438, time/batch = 0.088
    >> sample mode:
    The is a honoure to obey childly thousand indear take 
    ----------------------------------
    25598/46375 (epoch 68), train_loss = 1.438, time/batch = 0.069
    >> sample mode:
    The shall stand my broke and means are indeed.
    
    LADY C
    ----------------------------------
    25969/46375 (epoch 69), train_loss = 1.437, time/batch = 0.081
    >> sample mode:
    The shilat?
    
    Third Citizen:
    No, was like off me suffin
    ----------------------------------
    26340/46375 (epoch 70), train_loss = 1.437, time/batch = 0.085
    >> sample mode:
    The judges the followers well.
    
    MERCUTIO:
    The masted w
    ----------------------------------
    26711/46375 (epoch 71), train_loss = 1.436, time/batch = 0.099
    >> sample mode:
    The is will war, in England:
    Who would not she gost sp
    ----------------------------------
    27082/46375 (epoch 72), train_loss = 1.436, time/batch = 0.072
    >> sample mode:
    The say a parceire him: thy comfort him breath'd she? 
    ----------------------------------
    27453/46375 (epoch 73), train_loss = 1.436, time/batch = 0.064
    >> sample mode:
    The should keep thee than 'gue advice; and hover, and 
    ----------------------------------
    27824/46375 (epoch 74), train_loss = 1.435, time/batch = 0.093
    >> sample mode:
    The so? Juliet:
    Thou hadst he had should him to
    after 
    ----------------------------------
    28195/46375 (epoch 75), train_loss = 1.435, time/batch = 0.067
    >> sample mode:
    The suit
    Froward for it.
    
    SEBASTIAN:
    Well, forgot me.
    
    ----------------------------------
    28566/46375 (epoch 76), train_loss = 1.434, time/batch = 0.072
    >> sample mode:
    The shall not perfect,
    Were but I with an, not on than
    ----------------------------------
    28937/46375 (epoch 77), train_loss = 1.434, time/batch = 0.093
    >> sample mode:
    The shall a thousand life.
    
    ANGELO:
    And Romeo's sake.
    
    ----------------------------------
    29308/46375 (epoch 78), train_loss = 1.434, time/batch = 0.086
    >> sample mode:
    The shall are of my worth which queils hide their life
    ----------------------------------
    29679/46375 (epoch 79), train_loss = 1.433, time/batch = 0.087
    >> sample mode:
    The I, that I look
    But that I leo? some corpine with h
    ----------------------------------
    30050/46375 (epoch 80), train_loss = 1.433, time/batch = 0.075
    >> sample mode:
    The is my heated no body him;
    What must begest of Dras
    ----------------------------------
    30421/46375 (epoch 81), train_loss = 1.433, time/batch = 0.089
    >> sample mode:
    The is confider: then if I, shed, nouls they lips to h
    ----------------------------------
    30792/46375 (epoch 82), train_loss = 1.432, time/batch = 0.080
    >> sample mode:
    The is common our, or lease one house, at Plantagenes.
    ----------------------------------
    31163/46375 (epoch 83), train_loss = 1.432, time/batch = 0.078
    >> sample mode:
    The pition: forth the woung is come at
    Take her.
    
    ARCH
    ----------------------------------
    31534/46375 (epoch 84), train_loss = 1.432, time/batch = 0.067
    >> sample mode:
    The diended was touch dangerous lord, blood friends th
    ----------------------------------
    31905/46375 (epoch 85), train_loss = 1.432, time/batch = 0.071
    >> sample mode:
    The Datis, dewel, and my strake in Poding sent dike
    As
    ----------------------------------
    32276/46375 (epoch 86), train_loss = 1.431, time/batch = 0.088
    >> sample mode:
    The share is all the break: I will chack the confounte
    ----------------------------------
    32647/46375 (epoch 87), train_loss = 1.431, time/batch = 0.089
    >> sample mode:
    The yest
    As he should be a king:
    More be unstution whe
    ----------------------------------
    33018/46375 (epoch 88), train_loss = 1.431, time/batch = 0.067
    >> sample mode:
    The shall not took and seal'd ains foot.
    
    MERCUTIO:
    I'
    ----------------------------------
    33389/46375 (epoch 89), train_loss = 1.431, time/batch = 0.067
    >> sample mode:
    The ism means his mind; Northumberland Angelo it:
    So m
    ----------------------------------
    33760/46375 (epoch 90), train_loss = 1.430, time/batch = 0.083
    >> sample mode:
    The you doth goldeful look of face,
    This bark meiness,
    ----------------------------------
    34131/46375 (epoch 91), train_loss = 1.430, time/batch = 0.074
    >> sample mode:
    The shoow or she does sijes to turn of hastend,
    The ci
    ----------------------------------
    34502/46375 (epoch 92), train_loss = 1.430, time/batch = 0.088
    >> sample mode:
    The not shall before! your hoar? for some fled by the 
    ----------------------------------
    34873/46375 (epoch 93), train_loss = 1.430, time/batch = 0.080
    >> sample mode:
    The shall finding is,
    Or her poptined
    And a pacess an 
    ----------------------------------
    35244/46375 (epoch 94), train_loss = 1.429, time/batch = 0.108
    >> sample mode:
    The diz not my father of your house. The roral from Ve
    ----------------------------------
    35615/46375 (epoch 95), train_loss = 1.429, time/batch = 0.102
    >> sample mode:
    The she comes her?
    
    KING RICHARD III:
    So here along.
    
    
    ----------------------------------
    35986/46375 (epoch 96), train_loss = 1.429, time/batch = 0.069
    >> sample mode:
    The disortation alone's treath, gentle
    That thation a 
    ----------------------------------
    36357/46375 (epoch 97), train_loss = 1.429, time/batch = 0.068
    >> sample mode:
    The is I know' might whose day, god, Clifford be a wor
    ----------------------------------
    36728/46375 (epoch 98), train_loss = 1.428, time/batch = 0.079
    >> sample mode:
    The shall discred the sweat old hare to happyss one ip
    ----------------------------------
    37099/46375 (epoch 99), train_loss = 1.428, time/batch = 0.075
    >> sample mode:
    The shows we pit of this breast's rest, our, by Jud, w
    ----------------------------------
    37470/46375 (epoch 100), train_loss = 1.428, time/batch = 0.076
    >> sample mode:
    The shall did may be villes; Among him,
    One were I go,
    ----------------------------------
    37841/46375 (epoch 101), train_loss = 1.428, time/batch = 0.062
    >> sample mode:
    The lies my people, I do bela.
    
    MARCIUS:
    Go, though th
    ----------------------------------
    38212/46375 (epoch 102), train_loss = 1.428, time/batch = 0.066
    >> sample mode:
    The is news?
    
    Second Gentlemon to sickesting your fine
    ----------------------------------
    38583/46375 (epoch 103), train_loss = 1.427, time/batch = 0.072
    >> sample mode:
    The did have no eres to she looks him of my thrust the
    ----------------------------------
    38954/46375 (epoch 104), train_loss = 1.427, time/batch = 0.098
    >> sample mode:
    The Decreign,
    Beseech that we have is not, good golden
    ----------------------------------
    39325/46375 (epoch 105), train_loss = 1.427, time/batch = 0.084
    >> sample mode:
    The is not on deceives and part of Willoe 'twad-shost,
    ----------------------------------
    39696/46375 (epoch 106), train_loss = 1.427, time/batch = 0.074
    >> sample mode:
    The God.
    
    WARWICK:
    I'll gloe thee, with me and tribuna
    ----------------------------------
    40067/46375 (epoch 107), train_loss = 1.427, time/batch = 0.082
    >> sample mode:
    The Goode of the kept of his kind
    some day.
    
    FRIAR LAU
    ----------------------------------
    40438/46375 (epoch 108), train_loss = 1.426, time/batch = 0.079
    >> sample mode:
    The shrewly sire
    neilf.
    Unto two ere every of way the 
    ----------------------------------
    40809/46375 (epoch 109), train_loss = 1.426, time/batch = 0.142
    >> sample mode:
    The did perching too?
    
    WARWICK:
    And heirs a procession
    ----------------------------------
    41180/46375 (epoch 110), train_loss = 1.426, time/batch = 0.068
    >> sample mode:
    The should light
    me penite of men bendon as man;
    Into 
    ----------------------------------
    41551/46375 (epoch 111), train_loss = 1.426, time/batch = 0.071
    >> sample mode:
    The shalt twice Romeo, the truly.
    
    Shepherd:
    Thaw orde
    ----------------------------------
    41922/46375 (epoch 112), train_loss = 1.426, time/batch = 0.057
    >> sample mode:
    The did might weary roow-shires precifeless hard. How 
    ----------------------------------
    42293/46375 (epoch 113), train_loss = 1.426, time/batch = 0.050
    >> sample mode:
    The did.
    
    Thire were had you.
    
    COMINIUS:
    No, but farew
    ----------------------------------
    42664/46375 (epoch 114), train_loss = 1.425, time/batch = 0.053
    >> sample mode:
    The should have strong that my better.
    
    BUCKINGHAM:
    No
    ----------------------------------
    43035/46375 (epoch 115), train_loss = 1.425, time/batch = 0.049
    >> sample mode:
    The shall after.
    
    Third Comine!
    Take his head
    away's m
    ----------------------------------
    43406/46375 (epoch 116), train_loss = 1.425, time/batch = 0.052
    >> sample mode:
    The shake heart
    How alrent
    That do to thy.
    
    AUTOLYCUS:
    ----------------------------------
    43777/46375 (epoch 117), train_loss = 1.425, time/batch = 0.054
    >> sample mode:
    The should make take him enjoiftings much
    That what th
    ----------------------------------
    44148/46375 (epoch 118), train_loss = 1.425, time/batch = 0.056
    >> sample mode:
    The should canst contral Caligne; thy know
    Hoy
    Benishe
    ----------------------------------
    44519/46375 (epoch 119), train_loss = 1.425, time/batch = 0.058
    >> sample mode:
    The shall like father's son, have blood.
    
    Second Keeps
    ----------------------------------
    44890/46375 (epoch 120), train_loss = 1.425, time/batch = 0.061
    >> sample mode:
    The should bid sound his children,
    For who comes them,
    ----------------------------------
    45261/46375 (epoch 121), train_loss = 1.424, time/batch = 0.051
    >> sample mode:
    The shall give him.
    
    LADY RODI:
    Faith, had, fellows wi
    ----------------------------------
    45632/46375 (epoch 122), train_loss = 1.424, time/batch = 0.069
    >> sample mode:
    The she aspeed, is worthy brup.
    
    ESCALUSIO:
    Good knop 
    ----------------------------------
    46003/46375 (epoch 123), train_loss = 1.424, time/batch = 0.061
    >> sample mode:
    The set is knees and now; for with smell youwher your 
    ----------------------------------
    46374/46375 (epoch 124), train_loss = 1.424, time/batch = 0.053
    >> sample mode:
    The is withapumate,
    And he' rast it for this consess g
    ----------------------------------


## Want to learn more?

Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).

Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0120EN_DSX)This is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies.

### Thanks for completing this lesson!
Created by: <a href = "https://linkedin.com/in/saeedaghabozorgi"> Saeed Aghabozorgi </a></h4>
This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).
