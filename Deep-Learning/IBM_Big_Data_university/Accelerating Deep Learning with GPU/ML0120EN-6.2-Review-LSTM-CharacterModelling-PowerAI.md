
<a href="https://www.bigdatauniversity.com"><img src = "https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png" width = 300, align = "center"></a>


# <center> Text generation using RNN/LSTM (Character-level)</center>

<div class="alert alert-block alert-info">
<font size = 3><strong>In this notebook you will learn the How to use TensorFlow to create a Recurrent Neural Network</strong></font>
<br>    
- <a href="#intro">Introduction</a>
<br>
- <p><a href="#arch">Architectures</a></p>
    - <a href="#lstm">Long Short-Term Memory Model (LSTM)</a>

- <p><a href="#build">Building a LSTM with TensorFlow</a></p>
</div>
----------------

This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  
The RNN can then be used to generate text character by character that will look like the original training data. 

This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).




First, import the requiered libraries:


```python
import tensorflow as tf
import time
import codecs
import os
import collections
from six.moves import cPickle
import numpy as np
```

### Data loader
The following cell is a class that help to read data from input file.


```python
class TextLoader():
    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.encoding = encoding

        input_file = os.path.join(data_dir, "input.txt")
        vocab_file = os.path.join(data_dir, "vocab.pkl")
        tensor_file = os.path.join(data_dir, "data.npy")

        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):
            print("reading text file")
            self.preprocess(input_file, vocab_file, tensor_file)
        else:
            print("loading preprocessed files")
            self.load_preprocessed(vocab_file, tensor_file)
        self.create_batches()
        self.reset_batch_pointer()

    def preprocess(self, input_file, vocab_file, tensor_file):
        with codecs.open(input_file, "r", encoding=self.encoding) as f:
            data = f.read()
        counter = collections.Counter(data)
        count_pairs = sorted(counter.items(), key=lambda x: -x[1])
        self.chars, _ = zip(*count_pairs)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        with open(vocab_file, 'wb') as f:
            cPickle.dump(self.chars, f)
        self.tensor = np.array(list(map(self.vocab.get, data)))
        np.save(tensor_file, self.tensor)

    def load_preprocessed(self, vocab_file, tensor_file):
        with open(vocab_file, 'rb') as f:
            self.chars = cPickle.load(f)
        self.vocab_size = len(self.chars)
        self.vocab = dict(zip(self.chars, range(len(self.chars))))
        self.tensor = np.load(tensor_file)
        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))

    def create_batches(self):
        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))

        # When the data (tensor) is too small, let's give them a better error message
        if self.num_batches==0:
            assert False, "Not enough data. Make seq_length and batch_size small."

        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]
        xdata = self.tensor
        ydata = np.copy(self.tensor)
        ydata[:-1] = xdata[1:]
        ydata[-1] = xdata[0]
        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)
        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)


    def next_batch(self):
        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]
        self.pointer += 1
        return x, y

    def reset_batch_pointer(self):
        self.pointer = 0
```

### Parameters

Lets look at our dataset, with real parameters. 


```python
seq_length = 50 # RNN sequence length
batch_size = 60  # minibatch size, i.e. size of data in each epoch
num_epochs = 125 # you should change it to 50 if you want to see a relatively good results
learning_rate = 0.002
decay_rate = 0.97
rnn_size = 128 # size of RNN hidden state (output dimension)
num_layers = 2 #number of layers in the RNN
```

We download the input file, and print a part of it:


```python
!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt 
with open('input.txt', 'r') as f:
    read_data = f.read()
    print("-------------Sample text---------------")
    print (read_data[0:500])
    print("---------------------------------------")
f.closed
```

    2018-02-28 21:22:48 URL:https://public.boxcloud.com/d/1/70ZfVxpUuCjtop2jxamBbIvafVtpXBVOutgzmJjF8Q_MKkHltDG6EE-2Mco-YdZQ4NrNJyM2r0B4eYGzwq9L6_WyenuhELx_q-s-3dS1eyz6zHaqx_jZVE8DxApHn0H0lX1_17k6Hz6kzHghLZJ-tY58eh98npCUu3g1bkoQQvOjmHVRvFl9RvPwo7S49-wec-_gIfiCWbd-11bipZYZs-0H0v-TPMBizVm3zST1LZ3M4lOi7gDa3KjaGCHUM4ssA3bEFidCTjNUcFSsbosAKTWAPhvWVOyaNRw41iWX20sct_ypjtl2OZ2qieIAaz7qcR4NwVLCzWWioTjfgEa1uBgQHciCjwjRcHDX_44_Czt55dKk3wQtJKMBexIDHKyoEpQZSWQ9EWJ1jOLLViHR5osvGHhRzV4JyW7wGWHHFsRgZAON5nYQlW9Oz73JfAT1GO6FLnOThjBjBjw4txHf_bswhDWrzLj4BOEnLEEwszZGXKuPZbFXX8Ovy7vvP_m-dIxx20-VDydq9k6h-YFGtnuXpuQlB-hVJqJHG4yLxRD8bTd79Yr5cm9rZBDiaHwXxxFKRr-3i5JQhYCcuDHDWksPnq40UHb_J69j_mBn2PDvNPbe3VwN7BE7f73pTfTenjfzOfVvEwuDoLVIjkGn2qIz7rw_J-HcvMRzDC_59b3mlpSv74zJUbQstcXBLYKrMR8J69B81503QV-qKT290wA4oJMPZz1qBVKeHhxBABJ-gOCXDatCG8C7KA3-g72Ist48kgexjQsxoEDLxsbddrOpHX3u6ENdHUFY0FJOjKYMeTgbXcsO0wODg4btSFi6M-3q86LYxc1ZcrXo8ybgvlcjEwDfqFHwujCH8DXFMrTALncVN4NnqCP2eLMSw0Jdc5sY1q5kCMgbgjJTGojPJuEmYR91lqnaVNAE5WxbgJVmbXAUMeKBykUhjgK0HaOx8_d5P2ANLY3KhPdJou3Tza8s6rktba8EAiTJCtzDDcm8WtPgXoZWLAEE0oIxGYtrpY_Ut8pTZFOOC3sM7WOwFLmxTdAVFrDvLfNOosOHubPjFUQ5wYb38QlY6BYqrYxRcSeQXmYET_sgn-PqSYKcIqICVK52OrpWj38b4KIVNc-rL2JiTvQSfJ8QYf_oJNHKgqVU5pkEylx4yFIJ_5lOMoxJ5e_mUVZOQ5hZt7kF_ntF4rwR1eZhZPZ7eRyAG8Ft0_ZdE3L8TJjJzCmnxmcnLCg7DUqxfBbKXhe-xhaQVb5JfH_FT74Z4BKvHMB1K7l6FNqpAYoHz-YvOJMHO489Bx6Iwg98y8-AYkb9RccmaMGvuaGUu_1E6E4LPwG4E_o3VSRGkxqn2A3vrx4j-ZbQyAvYyw../download [1115393/1115393] -> "input.txt" [1]
    -------------Sample text---------------
    First Citizen:
    Before we proceed any further, hear me speak.
    
    All:
    Speak, speak.
    
    First Citizen:
    You are all resolved rather to die than to famish?
    
    All:
    Resolved. resolved.
    
    First Citizen:
    First, you know Caius Marcius is chief enemy to the people.
    
    All:
    We know't, we know't.
    
    First Citizen:
    Let us kill him, and we'll have corn at our own price.
    Is't a verdict?
    
    All:
    No more talking on't; let it be done: away, away!
    
    Second Citizen:
    One word, good citizens.
    
    First Citizen:
    We are accounted poor
    ---------------------------------------





    True



Now, we can read the data at batches using the __TextLoader__ class. It will convert the characters to numbers, and represent each sequence as a vector in batches:


```python
data_loader = TextLoader('', batch_size, seq_length)
vocab_size = data_loader.vocab_size
print ("vocabulary size:" ,data_loader.vocab_size)
print ("Characters:" ,data_loader.chars)
print ("vocab number of 'F':",data_loader.vocab['F'])
print ("Character sequences (first batch):", data_loader.x_batches[0])
```

    reading text file
    ('vocabulary size:', 65)
    ('Characters:', (u' ', u'e', u't', u'o', u'a', u'h', u's', u'r', u'n', u'i', u'\n', u'l', u'd', u'u', u'm', u'y', u',', u'w', u'f', u'c', u'g', u'I', u'b', u'p', u':', u'.', u'A', u'v', u'k', u'T', u"'", u'E', u'O', u'N', u'R', u'S', u'L', u'C', u';', u'W', u'U', u'H', u'M', u'B', u'?', u'G', u'!', u'D', u'-', u'F', u'Y', u'P', u'K', u'V', u'j', u'q', u'x', u'z', u'J', u'Q', u'Z', u'X', u'3', u'&', u'$'))
    ("vocab number of 'F':", 49)
    ('Character sequences (first batch):', array([[49,  9,  7, ...,  1,  4,  7],
           [19,  4, 14, ..., 14,  9, 20],
           [ 8, 20, 10, ...,  8, 10, 18],
           ..., 
           [21,  2,  0, ...,  0, 21,  0],
           [ 9,  7,  7, ...,  0,  2,  3],
           [ 3,  7,  0, ...,  5,  9, 23]]))


### Input and output


```python
x,y = data_loader.next_batch()
x
```




    array([[49,  9,  7, ...,  1,  4,  7],
           [19,  4, 14, ..., 14,  9, 20],
           [ 8, 20, 10, ...,  8, 10, 18],
           ..., 
           [21,  2,  0, ...,  0, 21,  0],
           [ 9,  7,  7, ...,  0,  2,  3],
           [ 3,  7,  0, ...,  5,  9, 23]])




```python
x.shape  #batch_size =60, seq_length=50
```




    (60, 50)



Here, __y__ is the next character for each character in __x__:


```python
y
```




    array([[ 9,  7,  6, ...,  4,  7,  0],
           [ 4, 14, 22, ...,  9, 20,  5],
           [20, 10, 29, ..., 10, 18,  4],
           ..., 
           [ 2,  0,  6, ..., 21,  0,  6],
           [ 7,  7,  4, ...,  2,  3,  0],
           [ 7,  0, 33, ...,  9, 23,  0]])



### LSTM Architecture
Each LSTM cell has 5 parts:
1. Input
2. prv_state
3. prv_output
4. new_state
5. new_output


- Each LSTM cell has an input layre, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of W2V/embedding, for each character/word.
- Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size
- An LSTM keeps two pieces of information as it propagates through time: 
    - __hidden state__ vector: Each LSTM cell accept a vector, called __hidden state__ vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. "num_units" is equivalant to "size of RNN hidden state". number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.
    - __previous time-step output__: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. 


#### num_layers = 2 
- number of layers in the RNN, is defined by num_layers
- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order.

### Defining stacked RNN Cell

__BasicRNNCell__ is the most basic RNN cell.


```python
cell = tf.contrib.rnn.BasicRNNCell(rnn_size)
```


```python
# a two layer cell
stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)
```


```python
# hidden state size
stacked_cell.output_size
```




    128



__state__ varibale keeps output and new_state of the LSTM, so it is a touple of size:


```python
stacked_cell.state_size
```




    (128, 128)



Lets define input data:


```python
input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50
input_data
```




    <tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>



and target data:


```python
targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50
targets
```




    <tf.Tensor 'Placeholder_1:0' shape=(60, 50) dtype=int32>



The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.

__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s). In this function, batch_size
representing the batch size.


```python
initial_state = stacked_cell.zero_state(batch_size, tf.float32) 
initial_state
```




    (<tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState/zeros:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>)



Lets check the value of the input_data again:


```python
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.05
session = tf.Session(config=config)
feed_dict={input_data:x, targets:y}
session.run(input_data, feed_dict)
```




    array([[49,  9,  7, ...,  1,  4,  7],
           [19,  4, 14, ..., 14,  9, 20],
           [ 8, 20, 10, ...,  8, 10, 18],
           ..., 
           [21,  2,  0, ...,  0, 21,  0],
           [ 9,  7,  7, ...,  0,  2,  3],
           [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)



### Embedding
In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix.

__Notice:__ The function `tf.get_variable()` is used to share a variable and to initialize it in one place. `tf.get_variable()` is used to get or create a variable instead of a direct call to `tf.Variable`. 


```python
with tf.variable_scope('rnnlm', reuse=False):
    softmax_w = tf.get_variable("softmax_w", [rnn_size, vocab_size]) #128x65
    softmax_b = tf.get_variable("softmax_b", [vocab_size]) # 1x65)
    #with tf.device("/cpu:0"):
        
    # embedding variable is initialized randomely
    embedding = tf.get_variable("embedding", [vocab_size, rnn_size])  #65x128

    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding
    # it creates a 60*50*[1*128] matrix
    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character
    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]
    # split: Splits a tensor into sub tensors.
    # syntax:  tf.split(split_dim, num_split, value, name='split')
    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]
    inputs = tf.split(em, seq_length, 1)
    # It will convert the list to 50 matrix of [60x128]
    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]
```

Lets take a look at the __embedding__, __em__, and __inputs__ variabbles:

Embedding variable is initialized with random values:


```python
session.run(tf.global_variables_initializer())
#print embedding.shape
session.run(embedding)
```




    array([[ 0.05525941, -0.04108109, -0.0534874 , ...,  0.15416272,
            -0.17470212,  0.13086985],
           [ 0.05147725, -0.09997387, -0.15362312, ...,  0.1473581 ,
             0.05163616,  0.11961724],
           [-0.165196  , -0.04615138,  0.03185973, ...,  0.1137801 ,
            -0.15309925,  0.13274284],
           ..., 
           [ 0.14837672,  0.02182835,  0.15672733, ...,  0.1442384 ,
            -0.04738255,  0.05780134],
           [ 0.01228298, -0.12109684, -0.02460645, ...,  0.01407979,
            -0.08725277,  0.07378058],
           [ 0.1005836 ,  0.1445405 , -0.07853239, ...,  0.00594032,
            -0.04689685,  0.08108713]], dtype=float32)



The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character


```python
em = tf.nn.embedding_lookup(embedding, input_data)
emp = session.run(em,feed_dict={input_data:x})
print (emp.shape)
emp[0]
```

    (60, 50, 128)





    array([[ 0.07705157,  0.01011713, -0.0196546 , ..., -0.15971006,
             0.04461862,  0.12552924],
           [-0.00641215, -0.17070927, -0.15095949, ..., -0.05185479,
            -0.06695218,  0.08254625],
           [ 0.15162243,  0.05060349,  0.11222057, ...,  0.03429984,
             0.1474825 ,  0.02977686],
           ..., 
           [ 0.05147725, -0.09997387, -0.15362312, ...,  0.1473581 ,
             0.05163616,  0.11961724],
           [ 0.03694269,  0.12493898,  0.17400326, ..., -0.16419008,
             0.03808586,  0.17489792],
           [ 0.15162243,  0.05060349,  0.11222057, ...,  0.03429984,
             0.1474825 ,  0.02977686]], dtype=float32)



Let's consider each sequence as a sentence of length 50 characters, then, the first item in __inputs__ is a [60x128] vector which represents the first characters of 60 sentences.


```python
inputs = tf.split(em, seq_length, 1)
inputs = [tf.squeeze(input_, [1]) for input_ in inputs]
inputs[0:5]
```




    [<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]



### Feeding a batch of 50 sequence to a RNN:

The feeding process for iputs is as following:

- Step 1:  first character of each of the 50 sentences (in a batch) is entered in parallel.  
- Step 2:  second character of each of the 50 sentences is input in parallel. 
- Step n: nth character of each of the 50 sentences is input in parallel.  

The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. 


```python
session.run(inputs[0],feed_dict={input_data:x})
```




    array([[ 0.07705157,  0.01011713, -0.0196546 , ..., -0.15971006,
             0.04461862,  0.12552924],
           [ 0.17278038, -0.10248629,  0.16270591, ...,  0.01052304,
             0.10807903, -0.07970233],
           [-0.04310955,  0.0118949 , -0.17100421, ..., -0.10583781,
            -0.0391558 , -0.12864716],
           ..., 
           [-0.00188383, -0.09817467, -0.03787635, ...,  0.07309337,
             0.01436044,  0.08870445],
           [-0.00641215, -0.17070927, -0.15095949, ..., -0.05185479,
            -0.06695218,  0.08254625],
           [ 0.14842515, -0.16605084, -0.13664955, ..., -0.0264917 ,
            -0.06234656,  0.11990272]], dtype=float32)



Feeding the RNN with one batch, we can check the new output and new state of network:


```python
#outputs is 50x[60*128]
outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')
new_state
```




    (<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_98:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_99:0' shape=(60, 128) dtype=float32>)




```python
outputs[0:5]
```




    [<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_1:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_3:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_5:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_7:0' shape=(60, 128) dtype=float32>,
     <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/cell_0/basic_rnn_cell/Tanh_9:0' shape=(60, 128) dtype=float32>]



Let's check the output of network after feeding it with first batch:


```python
first_output = outputs[0]
session.run(tf.global_variables_initializer())
session.run(first_output,feed_dict={input_data:x})
```




    array([[ 0.12752488, -0.01709103, -0.00462447, ..., -0.01304311,
            -0.03534542, -0.11524143],
           [-0.0167565 , -0.03815167, -0.02216692, ..., -0.03640661,
            -0.07662917,  0.01281242],
           [-0.01319886,  0.06621901,  0.12679377, ...,  0.01322465,
            -0.0539493 , -0.06238293],
           ..., 
           [-0.0611916 ,  0.00842849,  0.02041276, ...,  0.09005348,
             0.0330932 ,  0.14780387],
           [ 0.10197198,  0.06086342,  0.0463636 , ..., -0.02457   ,
            -0.06801114,  0.03547636],
           [ 0.01556346, -0.06645215, -0.06778235, ...,  0.03277918,
            -0.00267009,  0.06141061]], dtype=float32)



As it was explained, __outputs__ variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The __softmax_w__ shape is [rnn_size, vocab_size],whihc is [128x65] in our case. Threfore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the __softmax(output * softmax_w + softmax_b)__ for this purpose. The shape of the matrixis would be:

softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]

We can do it step-by-step:


```python
output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])
output
```




    <tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>




```python
logits = tf.matmul(output, softmax_w) + softmax_b
logits
```




    <tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>




```python
probs = tf.nn.softmax(logits)
probs
```




    <tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>



Here is the probablity of the next chracter in all batches:


```python
session.run(tf.global_variables_initializer())
session.run(probs,feed_dict={input_data:x})
```




    array([[ 0.01663614,  0.01673436,  0.013675  , ...,  0.01710156,
             0.01681982,  0.02042429],
           [ 0.02239257,  0.01944385,  0.01297136, ...,  0.01539698,
             0.01682866,  0.02038556],
           [ 0.0154259 ,  0.02335153,  0.01406547, ...,  0.01598461,
             0.02073482,  0.01512775],
           ..., 
           [ 0.01383667,  0.01656196,  0.01791571, ...,  0.01264198,
             0.01600973,  0.01277392],
           [ 0.01816371,  0.01561606,  0.0112087 , ...,  0.0157165 ,
             0.01484035,  0.02723002],
           [ 0.01838447,  0.02112635,  0.01413846, ...,  0.01727612,
             0.02136675,  0.01488798]], dtype=float32)



Now, we are in the position to calculate the cost of training with __loss function__, and keep feedng the network to learn it. But, the question is: what the LSTM networks learn?


```python
grad_clip =5.
tvars = tf.trainable_variables()
tvars
```




    [<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,
     <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,
     <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,
     <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,
     <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]



# All together
Now, let's put all of parts together in a class, and train the model:


```python
class LSTMModel():
    def __init__(self,sample=False):
        rnn_size = 128 # size of RNN hidden state vector
        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch
        seq_length = 50 # RNN sequence length
        num_layers = 2 # number of layers in the RNN
        vocab_size = 65
        grad_clip = 5.
        if sample:
            #print(">> sample mode:")
            batch_size = 1
            seq_length = 1
        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. 
        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)
        # model.cell.state_size is (128, 128)
        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)

        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name="input_data")
        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name="targets")
        # Initial state of the LSTM memory.
        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. 
        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size

        with tf.variable_scope('rnnlm_class1'):
            softmax_w = tf.get_variable("softmax_w", [rnn_size, vocab_size]) #128x65
            softmax_b = tf.get_variable("softmax_b", [vocab_size]) # 1x65
            embedding = tf.get_variable("embedding", [vocab_size, rnn_size])  #65x128
            inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)
            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]
            #inputs = tf.split(em, seq_length, 1)
                
                


        # The value of state is updated after processing each batch of chars.
        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')
        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])
        self.logits = tf.matmul(output, softmax_w) + softmax_b
        self.probs = tf.nn.softmax(self.logits)
        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],
                [tf.reshape(self.targets, [-1])],
                [tf.ones([batch_size * seq_length])],
                vocab_size)
        self.cost = tf.reduce_sum(loss) / batch_size / seq_length
        self.final_state = last_state
        self.lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)
        optimizer = tf.train.AdamOptimizer(self.lr)
        self.train_op = optimizer.apply_gradients(zip(grads, tvars))
    
    
    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):
        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))
        #print state
        for char in prime[:-1]:
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state:state}
            [state] = sess.run([self.final_state], feed)

        def weighted_pick(weights):
            t = np.cumsum(weights)
            s = np.sum(weights)
            return(int(np.searchsorted(t, np.random.rand(1)*s)))

        ret = prime
        char = prime[-1]
        for n in range(num):
            x = np.zeros((1, 1))
            x[0, 0] = vocab[char]
            feed = {self.input_data: x, self.initial_state:state}
            [probs, state] = sess.run([self.probs, self.final_state], feed)
            p = probs[0]

            if sampling_type == 0:
                sample = np.argmax(p)
            elif sampling_type == 2:
                if char == ' ':
                    sample = weighted_pick(p)
                else:
                    sample = np.argmax(p)
            else: # sampling_type == 1 default:
                sample = weighted_pick(p)

            pred = chars[sample]
            ret += pred
            char = pred
        return ret
```

### Creating the LSTM object
Now we create a LSTM model:


```python
with tf.variable_scope("rnn"):
    model = LSTMModel()
```

# Train usinng LSTMModel class
We can train our model through feeding batches:

<div class="alert alert-success alertsuccess" style="margin-top: 20px">
<font size = 3><strong>*You can run this cell if you REALLY have time to wait, or you are running it using PowerAI </strong></font>


What is PowerAI?

Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/DX0108EN-PowerAI).


__Notice:__ If you are running this notebook on PowerAI, it will automatically run on a GPU, otherwise it will use a CPU.


```python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher
        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))
        data_loader.reset_batch_pointer()
        state = sess.run(model.initial_state) # (2x[60x128])
        for b in range(data_loader.num_batches): #for each batch
            start = time.time()
            x, y = data_loader.next_batch()
            feed = {model.input_data: x, model.targets: y, model.initial_state:state}
            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)
            end = time.time()
        print("{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}" \
                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))
        with tf.variable_scope("rnn", reuse=True):
            sample_model = LSTMModel(sample=True)
            print ('----------------------------------')
            print ('SAMPLE GENERATED TEXT:')
            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))
            print ('----------------------------------')
```

    370/46375 (epoch 0), train_loss = 1.908, time/batch = 0.021
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The seds is deer, comest will stink!
    
    ThivenO I bow, and sugh what, allof or for of mund ant wouth more chout book.
    Noush eats sens
    The jucst spine onn an
    ----------------------------------
    741/46375 (epoch 1), train_loss = 1.754, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The chearow of me.
    
    FRIAPriD I will ane an He
    reate you grears: end go,
    Minish;
    God my head from theich.
    
    DUKE VINCENTIO:
    Yet no made thou head.
    
    VIUS:
    He
    ----------------------------------
    1112/46375 (epoch 2), train_loss = 1.682, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The were hall lordy livite your unother in Bmare Gropn?
    
    StONDENCE:
    Your date; and herreat in uphishe,
    Mone: I must an You have than rimens. Thou call.
    
    L
    ----------------------------------
    1483/46375 (epoch 3), train_loss = 1.642, time/batch = 0.022
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The preers: of.
    
    BUCKINGELE:
    Move of it;
    And let uward to cain,
    Do Even there dreaming,
    I not of your from can I will not nor live visheres of atter, in t
    ----------------------------------
    1854/46375 (epoch 4), train_loss = 1.617, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sten queen's. I will be a wither. Which you siler: if beous tase an a'll wherefore is
    with my handers trainus,
    pedy, is lies a brother my deart:
    Make 
    ----------------------------------
    2225/46375 (epoch 5), train_loss = 1.596, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The senger with requoungle with such them listed scufear, and some This life
    For you biter one unlowers would.
    
    CORIOLANUS:
    But you armether'd of dole,
    Bu
    ----------------------------------
    2596/46375 (epoch 6), train_loss = 1.580, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The heart and your man's you werk as my head:
    Urenasure us' your lineoug born to the king up
    And spemption guidget
    then, before to fines, the buss struck 
    ----------------------------------
    2967/46375 (epoch 7), train_loss = 1.565, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The see, swear the king bid thy trufs engergy of love,
    'Dile speaked indeeded old to your poison an and ill, incled; who hummore, sir, what is me, tell
    Re
    ----------------------------------
    3338/46375 (epoch 8), train_loss = 1.554, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The pritence,
    Let has come;
    Whom into dill no him.
    
    SICINIUS:
    Would appeasoness to kname marriagent, a foguest there not, throw no held he comfor with oat
    ----------------------------------
    3709/46375 (epoch 9), train_loss = 1.544, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The his younges' preach with him thou threen; but do
    that we speak
    Be for have pity as state descenier:
    Bat us we tuntinger fizer handman:
    cay be shaply f
    ----------------------------------
    4080/46375 (epoch 10), train_loss = 1.536, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The to mear. why, 'sown plain, 'tissinge.
    
    LEONTES:
    Romeo worder thing ever speaks:
    I'll be your body
    Mustinged his own from o east.
    
    PETRUCHIO:
    At her tu
    ----------------------------------
    4451/46375 (epoch 11), train_loss = 1.528, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The shed not sive bidst now heard and since;--too: where it is things might all the jest of of your fit brea meet,
    Believe abjeater, are them will pite hi
    ----------------------------------
    4822/46375 (epoch 12), train_loss = 1.521, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The ded,
    Of the moran eyes:-have you much. And you so accuest?
    
    DUKE OF YORK:
    How hold pin;
    And thou art, though your corrond,
    One, if your sarkin shall N
    ----------------------------------
    5193/46375 (epoch 13), train_loss = 1.516, time/batch = 0.022
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The thrat's cannouth, let hard:
    The loss Curmard a word with thy own? ale into in the are content more not night.
    
    Virst:
    And so contance? say like,
    With 
    ----------------------------------
    5564/46375 (epoch 14), train_loss = 1.511, time/batch = 0.023
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The take, my heaven?
    
    DUKE VINCENTIO:
    Look us?
    
    DUKE VINCENTIO:
    Madam; i' so gentleman,
    You know by men, as no use do it,
    With the very noble king with fa
    ----------------------------------
    5935/46375 (epoch 15), train_loss = 1.507, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The are stay'el home, Right, theresp for the bondage!--
    You must, disboth, and she stare.
    
    PrHont Menduce but me thou put his is.
    
    OXFORD:
    For a brave of 
    ----------------------------------
    6306/46375 (epoch 16), train_loss = 1.503, time/batch = 0.023
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Achers!
    
    GLOUCESTER:
    Alwarn,
    To ves for I blame,
    Ye's ever revil in earth of losget.
    
    Secomfence your coverguis;
    Angel then their of their deviter, if
    ----------------------------------
    6677/46375 (epoch 17), train_loss = 1.499, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The said fain by and lad, the offition is now, and
    it
    Freern'd, up majesty brave he stand?
    
    HASTINGS:
    So not plays!
    
    AUTOLEND:
    And I know takk: their whic
    ----------------------------------
    7048/46375 (epoch 18), train_loss = 1.495, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The I valiant.
    
    MENENIUS:
    Eaten'd me there's nobly much. Let my mind.
    
    LADY ANNE:
    How else you all his laft of suleen out, blood,
    As gued, well never will
    ----------------------------------
    7419/46375 (epoch 19), train_loss = 1.492, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sigiry to thy lengeak out; as a Though the looke; one to LawI.
    Angremen; stamp art. I will all Tower,
    Sir, and not to lose know'st gracien; but take h
    ----------------------------------
    7790/46375 (epoch 20), train_loss = 1.489, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The shows honour, that tell him together.
    
    MIRANDUG:
    When it saptime ritult was perposed with my son, what I art intender-hered uncle up therefore boy;
    Bu
    ----------------------------------
    8161/46375 (epoch 21), train_loss = 1.486, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The serve.
    
    LUCENTIO:
    Trair attend of came his since and streachor the moit's friend.
    
    BAPTISTA:
    I go, when make her good more, and read,
    I will raints, b
    ----------------------------------
    8532/46375 (epoch 22), train_loss = 1.484, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The bear the city dold gaid, tuis,
    That, me a main a now nature,
    Shall I must heen's may and callen things, I'll go them appear of slaughter,
    Is chascry w
    ----------------------------------
    8903/46375 (epoch 23), train_loss = 1.481, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The go.
    
    LEONTES:
    Braguldany.
    
    GREMIO:
    Be merchal, O cowd care them, there: what shall good for is adquend casicion ance. I have to my prisons along, was 
    ----------------------------------
    9274/46375 (epoch 24), train_loss = 1.479, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sent he resolved bear bet?
    
    PETER:
    Now dan sourtly place home.
    
    LADY CAPULET:
    Nay, servensy, bydise thee and sleen
    And I didst entreat,
    The suffer miz
    ----------------------------------
    9645/46375 (epoch 25), train_loss = 1.477, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sweet on by their court-discreations.
    
    HASTINGS:
    Thou whole Lady:
    But let not belited
    Que. Bain leace pritter, and you give me us reconce as your high
    ----------------------------------
    10016/46375 (epoch 26), train_loss = 1.475, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The burnamer welvels.
    Go say:
    But not tongue sun--year and their advancely the cause your join'd and that is yet lessmined: so not his greaticy of to hear
    ----------------------------------
    10387/46375 (epoch 27), train_loss = 1.472, time/batch = 0.021
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Cligracle.
    Look spall face the man?
    
    DUKE VINCENTIO:
    The fatt's give she good mournell, my send herefuld this
    England's ried; we give, hear him traito
    ----------------------------------
    10758/46375 (epoch 28), train_loss = 1.470, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The bawr.
    
    KING RICHARD III:
    Do you fiedy engaidly wretch'd sun of one an
    streight
    Wisher field; lile is much aught age; and my, and slests and Glouce.
    
    N
    ----------------------------------
    11129/46375 (epoch 29), train_loss = 1.468, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Arry: yel:
    Thinks I am wondegnock, thou live some alas,
    You mistrabling as arrieck, but, let me to my parks in the ballan clook, well, say's
    shall be 
    ----------------------------------
    11500/46375 (epoch 30), train_loss = 1.466, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The should know look I prepherd,
    For he hath good,
    Or marren comes, but life in Engmand, not,
    A tore wrese not sweet,
    And thou me laid,
    Hath mights,--
    
    Fi
    ----------------------------------
    11871/46375 (epoch 31), train_loss = 1.465, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The seat and, but make it orsel! freen--should had know any moved with throne, nor way. Thou shalt he more, God, and so, wilt amouchina, I do leifless cou
    ----------------------------------
    12242/46375 (epoch 32), train_loss = 1.463, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Mock of tem my tredua.
    
    BAPTISTA:
    Yea, yegpithe are full I drande be thine here; are old,
    Our poor bed,
    Wor my lord, withwealt; if thee sue the loss i
    ----------------------------------
    12613/46375 (epoch 33), train_loss = 1.461, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The right conief and gentle distress,
    Lustomely, sune,
    I some
    plight said lord and me-'I; my lord contrary! Ehe to coall.
    
    PIOSSTAN:
    And that you did an o
    ----------------------------------
    12984/46375 (epoch 34), train_loss = 1.460, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Doof Romeo, and thus take poor body
    Of a night; come thee must sight: must you the houses; mike me to see as you must fear you more gone,
    Should reste
    ----------------------------------
    13355/46375 (epoch 35), train_loss = 1.458, time/batch = 0.022
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The may love is alre was contempan;
    Been,
    To
    myhs king.
    
    BRUNE:
    Braves and impires.
    
    Clown:
    Why ward it and
    The entwall-gracious of counterfol! Sugnce out
    ----------------------------------
    13726/46375 (epoch 36), train_loss = 1.457, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The shout senven is done knawned gallo, and my friends:
    His conquage,
    And then, the should finding,
    Than the other to their queen of the spirit than let m
    ----------------------------------
    14097/46375 (epoch 37), train_loss = 1.456, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The several house as first: but it not Glows; and, 'light, sir, or marry, viother the an end done not the deformer Richard,
    The Marry
    For equary by apipe,
    ----------------------------------
    14468/46375 (epoch 38), train_loss = 1.455, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The slains, 'tis part,
    thou will not like me; thy reasone.
    
    GREMIO:
    He are the prince it woufure fault nothing parder, severe your suncining thee means ag
    ----------------------------------
    14839/46375 (epoch 39), train_loss = 1.453, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The banicher,
    'Tis saws be; and that do me to inpribale foldin her hath no to; he had grave son o' villan us the borrive estern, as name? why, despions, n
    ----------------------------------
    15210/46375 (epoch 40), train_loss = 1.452, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Clarence, and splitten me-dust?
    
    BRUTUS:
    Marry me.
    
    SAMPSON:
    You'll bear a little the stoothd-gladeration
    which all meaning, I have nest joy you trult
    ----------------------------------
    15581/46375 (epoch 41), train_loss = 1.451, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The serve;
    My best up some sake, if yout alted fire in a soul: night,
    And that is a
    veinth.
    
    ISABELLA:
    Did Glous and sunds therefore himself, Clarence!
    Fa
    ----------------------------------
    15952/46375 (epoch 42), train_loss = 1.450, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The so
    this women?
    
    KING RICHARD III:
    Who know;
    For know such are
    Resokest thousar; vicklocks not will not will not
    do clouds of them; of thou do bear pro
    ----------------------------------
    16323/46375 (epoch 43), train_loss = 1.449, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The should have the deed, as from we wise goes
    Our contents, is more dine, us wench
    The form;
    And King Rowful, how now, Tuegnest of my ark you.
    
    ISABELLA:
    ----------------------------------
    16694/46375 (epoch 44), train_loss = 1.448, time/batch = 0.021
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sea-banishment privoin.
    
    DUKE OF YORK:
    Would thrice for cruft be no threves, the Mercuous of me, ell, prepared her goods more to remember, sir, my lor
    ----------------------------------
    17065/46375 (epoch 45), train_loss = 1.447, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The just dance, if thou merce, when thou'll give me; only sit stopmast.
    
    GREGORY:
    When up from myself there is it is.
    Ay, say I have.
    Why, the looker of h
    ----------------------------------
    17436/46375 (epoch 46), train_loss = 1.446, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The humble of this framembity, was a hearing, as I will draw at truy
    scrood which he foune, nor I cannot, but let me the Lady I didst is knowledgerous the
    ----------------------------------
    17807/46375 (epoch 47), train_loss = 1.446, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The deep her state
    Bestrusterns withal? to fall of your promised, shoon summed the time.
    
    ROMEO:
    If dre is speak
    To you son,
    And thou wilt not mine and dr
    ----------------------------------
    18178/46375 (epoch 48), train_loss = 1.445, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The stand and patier, trufferous at my duty could he now the brother's daughter's most insterite, Montague.
    Now the mept depose for ever spilits feel is.
    
    ----------------------------------
    18549/46375 (epoch 49), train_loss = 1.444, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The honour of their four counterful:
    The last darest the house.
    3 KING HENRY VI
    Speak that I mignishal help, loge that Langaging, Mowly will; new comes?
    
    
    ----------------------------------
    18920/46375 (epoch 50), train_loss = 1.443, time/batch = 0.023
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The service,
    Frootreme ho!
    
    GRUMIO:
    O, his night?
    
    QUEEN ELIZABETH:
    No Imen are men:
    But, I will strickly to all in the purse.
    
    DUCHESS OF YORK:
    By Her;
    F
    ----------------------------------
    19291/46375 (epoch 51), train_loss = 1.442, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The go.
    
    MOPSA:
    Thou wouldsted him of shriftly be you not as enemies to king!
    Let you sight thee?
    
    SeKaving it?
    
    CORIOLANUS:
    The I am for his crimbly must
    ----------------------------------
    19662/46375 (epoch 52), train_loss = 1.442, time/batch = 0.017
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The borried, lords.
    
    First Second of so full of scovoughtal; every poosor
    Gue! libeth attend payst way rather Jowes?
    These thanks! I'll vent from manners
    
    ----------------------------------
    20033/46375 (epoch 53), train_loss = 1.441, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The say
    the tongue the end back. Why, but by the presence in owes fept no.
    And while I never with us,
    Reformerts,
    And the white, marriors the Dake to your
    ----------------------------------
    20404/46375 (epoch 54), train_loss = 1.440, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The name,
    Faith,
    Trow
    Lihatom, for pluck'd sometile bid to the'e of mercy and goodness.
    
    RICHARD:
    Lorswors his brot follow his sword, a'ty grave, thimough
    ----------------------------------
    20775/46375 (epoch 55), train_loss = 1.439, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The deep of its where I with brains of know news of her crown.
    
    First CAPLLAREA:
    I, markly considerward,
    if my burience means lined banish not: the fear y
    ----------------------------------
    21146/46375 (epoch 56), train_loss = 1.439, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The shore else him leave he and but as cae, which else
    Muse of 'I:
    They'll bequal hands,
    With us friend ground Carlius,
    I had tought
    To ere, my power, tra
    ----------------------------------
    21517/46375 (epoch 57), train_loss = 1.438, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Cliffas.
    
    QUEEN MARGARET:
    He are lead him ladr'd his inno's sacius widow; and sweet;
    Then not that his lerdiebportiss no more ciest.
    
    ROMEO:
    I please 
    ----------------------------------
    21888/46375 (epoch 58), train_loss = 1.438, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Dop eyes.
    I bid of owe roost, Sir Gentlemanlain, no; why fearn'd bihatom,
    By officence, chattering:
    And he know whom from the woe,
    And hath fain, and 
    ----------------------------------
    22259/46375 (epoch 59), train_loss = 1.437, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The command, as boy?
    
    Furses one fiest, expelite.
    
    QUEEN MARGARET:
    Thus to foul unjiding in to worth deserpey'd
    For thill! Prithee, play been abides; Thee
    ----------------------------------
    22630/46375 (epoch 60), train_loss = 1.436, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The war, got purbest Warwick, my lowry;
    And prizal her.
    No, my master liege,
    Which our isle speak:
    Marriers.
    Prince for bancals, my good sun, my grow
    If t
    ----------------------------------
    23001/46375 (epoch 61), train_loss = 1.436, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The servict.
    O, worthor Romeo.
    
    SEBASTIAN:
    Bost ill wake upon young,
    And
    missemp-that hast's earth. Away incience.
    
    GLOUCESTER:
    Wisely dase, as home that 
    ----------------------------------
    23372/46375 (epoch 62), train_loss = 1.435, time/batch = 0.022
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sea:
    MaKitone throwns,
    Which in your madness by runger knipant that, to conchament. Isk you Augot. Wish, whish'd thine.
    
    DUCHESS OF YOPSLO:
    Then, merl
    ----------------------------------
    23743/46375 (epoch 63), train_loss = 1.435, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Toway.
    
    MARIANA:
    Rest love,--
    A thousand graves,
    Is cast.
    
    NORTELAND:
    How lade, sir; upon your fet's with to the writion of my Rame?
    
    First Lord Beast
    ----------------------------------
    24114/46375 (epoch 64), train_loss = 1.434, time/batch = 0.017
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The guouther troubled, and estrees,
    It loves me warm?
    
    Clown:
    Say you call'd with thick!
    This brother of our ofter's pursuagh?
    
    Second Murry Wife and offi
    ----------------------------------
    24485/46375 (epoch 65), train_loss = 1.434, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sprike our for's honour's botteny,
    More of noble women,
    Are this murderer
    A vow
    Prediel!
    
    POMPEY:
    Hark a priest be thy stops of your other for that ha
    ----------------------------------
    24856/46375 (epoch 66), train_loss = 1.433, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Richmond.
    What boy, this pertains after.
    
    SEBASTIAN:
    No all,
    I think you?
    
    SORELLO:
    Katharing his pacious war black'd that bury you; with my patest hi
    ----------------------------------
    25227/46375 (epoch 67), train_loss = 1.433, time/batch = 0.021
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The AUll:
    Your pastards; for call'd dobury infectous, brave now, as the hence, a son without ungrutt and with a suffer mine honour! Our, before, I might a
    ----------------------------------
    25598/46375 (epoch 68), train_loss = 1.433, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The brace.
    
    Clown:
    I would request!
    
    ISCAURIA
    A growers, but she had well, could fall in murderer! Be come; extempe.
    Thy man.
    
    GLOUCESTER:
    I see the pup h
    ----------------------------------
    25969/46375 (epoch 69), train_loss = 1.432, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The go wearly after thing for until last our houring widool,
    For on to prainor and thy heart;
    The plunanuss was by a
    lion me-sir, better use of Richard, m
    ----------------------------------
    26340/46375 (epoch 70), train_loss = 1.432, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Sincing; with thy duke, in hearing or not are not calls
    fook
    His seems, adomy hastes: my corse.
    
    WARWICK:
    Prowled
    Is peries mana:
    And he is our marria
    ----------------------------------
    26711/46375 (epoch 71), train_loss = 1.431, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The stufjest him count, by the powen him; but thy borrivorro would tribune.
    
    LEONTES:
    Ay, say God bloiding
    And leave upon than you him.
    
    SEBASTIAN:
    How pl
    ----------------------------------
    27082/46375 (epoch 72), train_loss = 1.431, time/batch = 0.021
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Marcius.
    
    JULIET:
    Welcime, that earld misgui bury
    The world, that's after'd his indeed
    Affeigness?
    Howard Johth and pale, let's fault. Dohthers! are c
    ----------------------------------
    27453/46375 (epoch 73), train_loss = 1.431, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sight?
    
    LADY GLOUSMEY:
    All womer Gloucester's heart done?
    
    GLOUCESTER:
    There than dry 'rremery the Earts to gelded
    The morning droarder, feat more men
    ----------------------------------
    27824/46375 (epoch 74), train_loss = 1.430, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The seen husband's way they find woman;
    I might and you but stays all me,
    Came wa,
    Biried and in this day.
    
    ESCALUS:
    Solly they be a will in bound, home i
    ----------------------------------
    28195/46375 (epoch 75), train_loss = 1.430, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The subject
    Take passar about such his chamber'd, to sit, there, I come.
    
    All,
    Pelful of amilie, for I hear me, and mert done!
    I you by by these sense!
    
    N
    ----------------------------------
    28566/46375 (epoch 76), train_loss = 1.430, time/batch = 0.023
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The are
    Cup eyes before him.
    
    ARIANA:
    
    KING EDWARD IV:
    I pray you home: cousin, if I mine; and faults prayers ploody born!
    The biat, brother thee
    To them 
    ----------------------------------
    28937/46375 (epoch 77), train_loss = 1.429, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Duke of Gloucester:
    Inder:
    Mistress at Lucentio's life.
    
    YORK:
    There like all, bewite.
    
    KING RICHARD II:
    If all this wrongs ur your day this none a we
    ----------------------------------
    29308/46375 (epoch 78), train_loss = 1.429, time/batch = 0.017
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Dion:
    He sing?
    
    RATCLIFF:
    Come opperation.
    
    GLOUCESTER:
    Go bids an hour that say is miny that not upon it enjoy, his good of thy liagom me:
    Read'st th
    ----------------------------------
    29679/46375 (epoch 79), train_loss = 1.429, time/batch = 0.017
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The service
    As answer, the mother's good man, and very wair!
    The tender your honour, and I bodiafest the virgue her, city got beat friench'd, a lady,
    To s
    ----------------------------------
    30050/46375 (epoch 80), train_loss = 1.429, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The banis words:
    I'll cult be not be to their grace cominguks die: overfect,
    Since we searn.
    
    DUKE OF YORK:
    We in the acon?
    
    ANGELO:
    Since you thought the
    ----------------------------------
    30421/46375 (epoch 81), train_loss = 1.428, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The soot
    To dar; good determily well.
    Against the sease your rutter.
    
    POMPEY:
    My Lord opid doth shall voice hither;
    For came from him be knaps humb'd
    Entr
    ----------------------------------
    30792/46375 (epoch 82), train_loss = 1.428, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The great is mine;
    Well, for you
    though any that did soon take of York.
    All chatis,
    Whilst rather teffret, my holy of Chrimblank!
    Now, I will not.
    
    Second
    ----------------------------------
    31163/46375 (epoch 83), train_loss = 1.428, time/batch = 0.022
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The shame thou cry throses to the ad.
    
    CORIOLANUS:
    My Lorder:
    With what inly nose;
    Richard else!
    What proud:
    Against sustops a tyrant be want this name.
    
    
    ----------------------------------
    31534/46375 (epoch 84), train_loss = 1.428, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The counsels! with
    a brother were perform'd, alono, be I think it mean seed mans that liars of child.
    
    GREGORY:
    Four say his brother hallows
    Made it; and 
    ----------------------------------
    31905/46375 (epoch 85), train_loss = 1.427, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The second or duty, lords, grow; and do wouldst heir,
    Thou before dead sweet
    Is came lady by the stale, and his like looks;
    Returning dismall.
    
    First Citi
    ----------------------------------
    32276/46375 (epoch 86), train_loss = 1.427, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The man;
    The purged no, now, thou can he's both of ours;
    So seal
    And lie,
    Being the put to my gate,
    That for the loss in lames heary, as if by a resoletes
    ----------------------------------
    32647/46375 (epoch 87), train_loss = 1.427, time/batch = 0.024
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The good will in the duke.
    O, yet a prince of your
    dones:
    For it is he which thou have deep, gen the pale he hath longent them in my uports,
    Against under
    ----------------------------------
    33018/46375 (epoch 88), train_loss = 1.427, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Tranioline, feeged
    Be it her put the venesher:
    No little
    On
    as like
    Much of hope
    Till there
    Cupoud corrumptoods
    Are not feedured bloody fair
    but late;
    ----------------------------------
    33389/46375 (epoch 89), train_loss = 1.427, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Rome.
    Meavones your eye,
    Pray; sir.
    
    MENENIUS:
    It who will true this heads;
    And would bids upon my father's fall your honour that was a time!
    
    First S
    ----------------------------------
    33760/46375 (epoch 90), train_loss = 1.426, time/batch = 0.023
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The services:
    Nay,
    And to take and fles veir neveroughters.--Julies you were me in pedsomes,--
    Trumpt, mark to me, I know that loss you are for now,
    Being
    ----------------------------------
    34131/46375 (epoch 91), train_loss = 1.426, time/batch = 0.017
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The no the
    Free for loverey: what would confusion;
    And, few I would take his folen thyself.
    
    MENENIUS:
    You are deserve should stand would said ears that m
    ----------------------------------
    34502/46375 (epoch 92), train_loss = 1.426, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Murdia
    Bestain: came ageis, but it be any field, are I.
    In thin foes their army.
    
    RATCLINEN:
    Youn trial in me; and be a brother, well.
    
    BIONDELLO:
    Sho
    ----------------------------------
    34873/46375 (epoch 93), train_loss = 1.426, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The but my fobbedy;
    And say I slave our scares,
    Which is got Artian from two dangs,
    Are we stand blood out.
    
    DUCHESS OF YORK:
    O looking a pleasure
    To his 
    ----------------------------------
    35244/46375 (epoch 94), train_loss = 1.426, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The supply joy'd?
    
    VIRGILIA:
    Away the's neive,
    Our grave:
    You are speaks most highwands doze two state of my mistress
    Kishing tribuly breath, and wilt be 
    ----------------------------------
    35615/46375 (epoch 95), train_loss = 1.425, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The brock! O coursed in my father's speech,
    These in the would to the lord, 'tis the face, if you.
    
    LARTIUS:
    His states,
    And now is the malica heart's top
    ----------------------------------
    35986/46375 (epoch 96), train_loss = 1.425, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The York.
    
    KING RICHARD III:
    Ever we but it.
    
    GLOUCESTER:
    Mark! were I say
    And thine elmen for a moice did here begon it.
    Here is not
    to you
    have my life 
    ----------------------------------
    36357/46375 (epoch 97), train_loss = 1.425, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The bring and you.
    
    ESCALUS:
    now 'tis helpant succiess keep of wo caple scorn'd with them.
    3 KING HENRY VI
    
    PAULINA:
    Please; wither prick!
    
    MARCIUS:
    'Thap
    ----------------------------------
    36728/46375 (epoch 98), train_loss = 1.425, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The seat me for you.
    
    SLY:
    Fair majaster, have from thy matter Richard against to the Lords, I saw,
    And thou, pain,
    Your trued
    Becounnel socks serve is.
    B
    ----------------------------------
    37099/46375 (epoch 99), train_loss = 1.425, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The son-graped; and the hale near at pleased me.
    
    ESCALUS:
    Upon my curs show'd
    Of a pride.
    
    GREMIO:
    It at it, but my Lords:
    Thou hast in any cannot Peter 
    ----------------------------------
    37470/46375 (epoch 100), train_loss = 1.425, time/batch = 0.017
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Achine and
    That heard and what you, shaped him if grame.
    On ficked that indegues home. Than my belly
    And prove to some sire: if you eniup again.
    Hark 
    ----------------------------------
    37841/46375 (epoch 101), train_loss = 1.424, time/batch = 0.028
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The brid wint.
    I'll injurions.
    
    ESCKIBASA:
    God knaving too most cobs:
    'Tis night, on the runhy unmigro. O, he revenges, old gentle. Anonest thou dispose.
    
    ----------------------------------
    38212/46375 (epoch 102), train_loss = 1.424, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The none, here?
    
    CAzTESSUGHIZAPrable appart with name
    To raid, see these passing mercy of sleep me?'
    
    Nurse:
    Aqual the time
    to love us valiant it?
    Since l
    ----------------------------------
    38583/46375 (epoch 103), train_loss = 1.424, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The shall scarf and all all in rejoadious more awake to Rome, sir, this I do dears wife, had condition of me;
    But what thou, for her.
    
    CATESBY:
    I would yo
    ----------------------------------
    38954/46375 (epoch 104), train_loss = 1.424, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Very to me, Greling Clifford,
    And sleep of sivtering al;
    Let him heavy it in me! lay my precily
    Or Catesbit lust?
    
    LEONTES:
    You can if theep;
    Is not b
    ----------------------------------
    39325/46375 (epoch 105), train_loss = 1.424, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The swords
    Armiss of my moint,
    'File my lord, whilst no more brailly a proforess.
    All hate dead:
    Yet for Stock! do this, I beseech your slow?
    
    SICINIUS:
    P
    ----------------------------------
    39696/46375 (epoch 106), train_loss = 1.424, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The before thee, sweept you shall not had cannot safety used, and my swept! the kingdom whom I say, tush.
    
    DUKE VINCENTIO:
    Art, although thou canst I will
    ----------------------------------
    40067/46375 (epoch 107), train_loss = 1.424, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The spite would heaven, for the compless is the Dordest
    would not to the ascied-distrough no delicted his anothindgs:
    Lead strange can wall so, I know, ha
    ----------------------------------
    40438/46375 (epoch 108), train_loss = 1.423, time/batch = 0.028
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The shall you.
    
    PAULINA:
    I have as too with theirs my fublic:
    To Lanicy for the highness that vayst no bears
    Save,
    My breast.
    
    MENENIUS:
    What know of: wha
    ----------------------------------
    40809/46375 (epoch 109), train_loss = 1.423, time/batch = 0.023
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The woman!
    
    LADY ANNE:
    Return my time, to live, no, I gods by these horse:
    Recover of devation hath so, and not the law and the very soldy o'erther as fel
    ----------------------------------
    41180/46375 (epoch 110), train_loss = 1.423, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The be well sleeving less,
    And do have such itself.
    
    PRINCE:
    Come in my lat:
    The service,
    For my nobles and sharp but of the thing.
    
    MERCUTIO:
    What, aligr
    ----------------------------------
    41551/46375 (epoch 111), train_loss = 1.423, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The steal tise conceive your honour, he?
    
    VOLUMNIA:
    Well,
    That I.
    Whellayn,--
    
    SLY:
    Ay, looked me of his blood to see in his.
    
    HENRY BOLINGBROKE:
    We know 
    ----------------------------------
    41922/46375 (epoch 112), train_loss = 1.423, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The brother,
    comforates, I do shallows to so teach me,
    Feice same forget York.
    
    FRIAR LAURENEE:
    Romainst
    shed popt, impurity with the office, but them.
    
    G
    ----------------------------------
    42293/46375 (epoch 113), train_loss = 1.423, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Citizen:
    You, my lord: you keep Hereford, they call Herry,
    The liege will fortest belips may ER knees, because and encounter?
    
    MISTRESS OVERDONE:
    Even
    ----------------------------------
    42664/46375 (epoch 114), train_loss = 1.423, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The may, and the last jack'd,
    Where we aways mabrooks being infirmity and load!
    
    CLIFFORD:
    I have worry,
    That I had'
    Has suspable?
    
    POMPEY:
    
    CORIOLANUS:
    H
    ----------------------------------
    43035/46375 (epoch 115), train_loss = 1.423, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The tell.
    
    DUKE VINCENTIO:
    I give Miliggue in our sire as in eyes, redeem her tonsparith, nor which thou tageent: gilly, upon this hour so less and by mos
    ----------------------------------
    43406/46375 (epoch 116), train_loss = 1.422, time/batch = 0.020
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The swainst thou but sweet.
    
    ESCALUS:
    Your purjacted your hands
    What putors cole city;
    And thou may groan's night
    Grey'd is a counterfort of the
    dream, di
    ----------------------------------
    43777/46375 (epoch 117), train_loss = 1.422, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The boldst there, who thine it with me all.
    
    CORIOLANUS:
    Is thy scket myself.
    If I say, good Citizen!
    
    BRUTUS:
    Trangoster of walks, or this sword to light
    ----------------------------------
    44148/46375 (epoch 118), train_loss = 1.422, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The sure feast thy heave the lamb! advicons play
    And beseeps, unto the slanded:
    Brown yet good might been with sight be divers wrong'd.
    
    CAMILLO:
    Enguce I
    ----------------------------------
    44519/46375 (epoch 119), train_loss = 1.422, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Most up bornes:
    A revery helps
    And not: as I warrant service; that's a crowns:
    Not on, my soul',
    But bitter ser?
    
    SICINIUS:
    Where I thank tremlet-henc
    ----------------------------------
    44890/46375 (epoch 120), train_loss = 1.422, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The bengmant and tainnends are stranger. What is people unfage;
    On.
    
    AUFIDIUS:
    And be alleed, but you
    Wifes,--
    
    MIRETIUS:
    How thou hast for kill that meta
    ----------------------------------
    45261/46375 (epoch 121), train_loss = 1.422, time/batch = 0.019
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Rome; Capider, bodiel co, like at thee more,
    Hasty remembrance home and that names, away. You weepen, grieve him.
    
    CORIOLANUS:
    'This deeds.
    
    First Cit
    ----------------------------------
    45632/46375 (epoch 122), train_loss = 1.422, time/batch = 0.022
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The good is so weply ask thou womfort diknitiful blood!
    And she's in. Speald'd your boot
    Horter under refulle. I'll is her.
    
    LADY ANNE:
    Privides to be dot
    ----------------------------------
    46003/46375 (epoch 123), train_loss = 1.422, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The Rorak;
    But, and thou,
    Like no commons do,.
    
    KING RICHARD III:
    Turn health with my generance, to glow his honour; thou may, and Montugrect of SomeNe th
    ----------------------------------
    46374/46375 (epoch 124), train_loss = 1.422, time/batch = 0.018
    ----------------------------------
    SAMPLE GENERATED TEXT:
    The seat briblnosed
    When I lucherable more: here's since o'er your inucentious daughters to come count; where' meet on by you,
    God and so; To depend the a
    ----------------------------------


## Want to learn more?

Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).



### Thanks for completing this lesson!


<h3>Authors:</h3>
<article class="teacher">
<div class="teacher-image" style="    float: left;
    width: 115px;
    height: 115px;
    margin-right: 10px;
    margin-bottom: 10px;
    border: 1px solid #CCC;
    padding: 3px;
    border-radius: 3px;
    text-align: center;"><img class="alignnone wp-image-2258 " src="https://media.licdn.com/mpr/mpr/shrinknp_400_400/AAEAAQAAAAAAAAyFAAAAJGJlM2I2MmQzLTkxOWQtNDVhZi1hZGU0LWNlOWQzZDcyYjQ3ZA.jpg" alt="Saeed Aghabozorgi" width="178" height="178" /></div>
<h4>Saeed Aghabozorgi</h4>
<p><a href="https://ca.linkedin.com/in/saeedaghabozorgi">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>
</article>

##### Reference
This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).
