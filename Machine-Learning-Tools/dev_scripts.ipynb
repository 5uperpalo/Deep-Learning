{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "done with startup\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## SET ENV. PARAMETERS & CREATE INIT. SESSIONS  ##\n",
    "##################################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda-4.4.0/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-oracle/jre\"\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/home/big-dama/pavol/moa-release-2018.6.0/lib/moa.jar\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# distributed mode - only \"--deploy-mode client\" is available in jupyter, you have to run the the script byt \"submit spark\" in order to have \"--deploy-mode cluster\"\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master yarn --deploy-mode client --jars /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar --driver-class-path /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar pyspark-shell\"\n",
    "# local testing mode\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[2] --jars /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar --driver-class-path /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar pyspark-shell\"\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()#.set(\"spark.jars\", \"/home/big-dama/pavol/moa-release-2018.6.0/lib/moa.jar\")\n",
    "conf.setAppName('dev_n_testing')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"done with startup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[3.6619E7,86533.0...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "## DATASET LOADING  ##\n",
    "######################\n",
    "\n",
    "from scipy.io import arff\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#loadarff creates scipy record array, [0] - data, [1] - labels; in the following I extract labels and everything except labels\n",
    "data = arff.loadarff('/home/big-dama/pavol/mawi_datasets/mawi_data_1_long.arff')\n",
    "pd_df = pd.DataFrame(data=data[0], columns=data[1])\n",
    "pd_df.label = pd_df.label.astype(float)\n",
    "\n",
    "#convert pandas data frame to spark dataframe\n",
    "sqlContext = SQLContext(sc)\n",
    "spark_df = sqlContext.createDataFrame(pd_df)\n",
    "\n",
    "#create features column for further machine learning processing\n",
    "assembler = VectorAssembler(inputCols=list(pd_df)[:-1], outputCol=\"features\")\n",
    "spark_df_final = assembler.transform(spark_df)\n",
    "spark_df_final.select(\"features\", \"label\").show(n=1,truncate=True)\n",
    "#the label column is detected as string and needs to be change to doubletype for further processing\n",
    "spark_df_final = spark_df_final.withColumn(\"label\", spark_df_final[\"label\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 5.79140325166e+24\n",
      "Cluster Centers: \n",
      "[  6.50167161e+07   1.24757310e+05   6.00000000e+01   5.16285419e+02\n",
      "   1.51400000e+03   4.11933226e+05   6.41333806e+02   6.00000000e+01\n",
      "   6.00000000e+01   6.00000000e+01   6.00000000e+01   6.00000000e+01\n",
      "   6.00000000e+01   6.00129032e+01   6.80000000e+01   1.36383226e+03\n",
      "   1.51352258e+03   1.51400000e+03   1.51400000e+03   1.51400000e+03\n",
      "   3.65824065e-01   7.23870968e+00   1.00000000e+00   6.04364710e+00\n",
      "   6.67096774e+01   2.86910194e+01   5.12838710e+00   1.00000000e+00\n",
      "   1.00000000e+00   1.00000000e+00   1.00000000e+00   1.19354839e+00\n",
      "   3.61290323e+00   4.25806452e+00   6.00000000e+00   6.00000000e+00\n",
      "   7.84516129e+00   1.71935484e+01   1.90903226e+01   2.45741935e+01\n",
      "   4.13891548e-01   2.79290323e+01   4.58255548e+02   2.13845806e+03\n",
      "   3.97764129e+05   6.30037742e+02   3.19483871e+01   3.20000000e+01\n",
      "   3.20000000e+01   3.20000000e+01   3.24387097e+01   3.65677419e+01\n",
      "   3.74516129e+01   5.24903226e+01   1.18445806e+03   1.49947097e+03\n",
      "   1.50000000e+03   1.50000000e+03   1.50000000e+03   3.93285871e-01\n",
      "   0.00000000e+00   7.79517355e+01   2.54954839e+02   2.62692774e+03\n",
      "   5.10392194e+01   2.62774194e+01   4.26000000e+01   4.83032258e+01\n",
      "   5.03032258e+01   5.16258065e+01   5.27483871e+01   5.39032258e+01\n",
      "   5.86709677e+01   7.22903226e+01   1.29574194e+02   2.29703226e+02\n",
      "   2.47961290e+02   2.51490323e+02   6.24406000e-01   9.36181806e-01\n",
      "   6.38175419e-02   2.50572258e+03   5.38387097e+03   8.30967742e+02\n",
      "   2.77074735e-01   8.70258065e+01   3.04683484e-01   3.18064516e+00\n",
      "   2.87852645e+04   6.55214323e+04   6.89895935e+08   2.62532903e+04\n",
      "   2.44903226e+01   6.67870968e+01   7.81354839e+01   7.88903226e+01\n",
      "   1.33122581e+02   1.72948387e+02   2.16696774e+02   3.44049484e+04\n",
      "   5.33518000e+04   5.96987548e+04   6.21872516e+04   6.33585484e+04\n",
      "   6.47033290e+04   5.49698968e-01   9.29032258e+00   2.19865419e+04\n",
      "   6.55134774e+04   6.07002581e+08   2.46168129e+04   4.10903226e+01\n",
      "   7.95032258e+01   8.00000000e+01   8.00000000e+01   8.23419355e+01\n",
      "   8.51161290e+01   1.35135484e+02   6.81898710e+03   4.92846903e+04\n",
      "   5.77540194e+04   6.04834452e+04   6.19653548e+04   6.39913742e+04\n",
      "   5.24890323e-01   1.01290323e+01   1.85161290e+00   1.59229955e+01\n",
      "   3.02438710e+02   3.18866258e+01   4.74150645e+00   2.00000000e+00\n",
      "   2.00000000e+00   1.06322581e+01   1.52000000e+01   1.54580645e+01\n",
      "   1.57290323e+01   1.57290323e+01   1.59096774e+01   1.60000000e+01\n",
      "   1.81290323e+01   2.36903226e+01   2.40000000e+01   2.40000000e+01\n",
      "   2.60462194e-01   1.02658477e+00   1.08236187e-04   9.91709161e-03\n",
      "   1.22693735e-04   1.59112903e-06   9.09456581e-02   6.68463484e-02\n",
      "   2.29822581e-06   0.00000000e+00   6.15108903e+02   1.46000000e+03\n",
      "   4.65635226e+05   6.82169935e+02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.96451613e+02   1.41607097e+03   1.45494194e+03\n",
      "   1.45984516e+03   1.46000000e+03   1.46000000e+03   3.09149677e-01\n",
      "   0.00000000e+00   3.58282129e+04   3.37420271e+07   2.42625484e+11\n",
      "   4.77206516e+05   2.61935484e+01   3.30000000e+01   5.35870968e+01\n",
      "   8.09483871e+01   1.76077419e+02   3.36090323e+02   6.16541935e+02\n",
      "   4.47358710e+03   2.27437290e+04   7.60110968e+04   9.10655677e+04\n",
      "   1.13635800e+05   2.96798348e+05   6.73102839e-01   1.61305806e+03\n",
      "   9.56993548e+02   8.21630323e+03   1.38810445e-01   8.92028387e+03\n",
      "   1.54154987e-01   1.17161290e+01   2.40016323e+04   6.55181290e+04\n",
      "   4.95330581e+08   2.22278065e+04   4.22000000e+01   5.28000000e+01\n",
      "   5.47290323e+01   1.38400000e+02   8.04503226e+02   1.55820000e+03\n",
      "   2.87629677e+03   1.74495742e+04   4.54158129e+04   5.61947871e+04\n",
      "   6.04697613e+04   6.20576968e+04   6.40265032e+04   6.51353419e-01\n",
      "   1.58451613e+01   2.81648219e+04   6.54748903e+04   5.40253355e+08\n",
      "   2.32207226e+04   5.25612903e+01   5.34516129e+01   5.81096774e+01\n",
      "   1.21729032e+02   9.87948387e+02   2.58226452e+03   4.63491613e+03\n",
      "   2.51019806e+04   5.15170129e+04   5.90605097e+04   6.26143355e+04\n",
      "   6.37738516e+04   6.47604903e+04   6.69306000e-01   1.05108258e+04\n",
      "   2.40732581e+04   3.38819806e+09   1.55629206e-01   2.78338903e+09\n",
      "   2.09521755e-01   2.18991548e-01   6.48420516e-01   7.39275226e-02\n",
      "   7.12961961e-03]\n",
      "[  5.42532759e+07   1.03877257e+05   6.00000000e+01   5.14161424e+02\n",
      "   1.51400000e+03   4.10071993e+05   6.39724777e+02   6.00000000e+01\n",
      "   6.00000000e+01   6.00000000e+01   6.00000000e+01   6.00000000e+01\n",
      "   6.00000000e+01   6.00633037e+01   8.40583581e+01   1.30000198e+03\n",
      "   1.51384125e+03   1.51400000e+03   1.51400000e+03   1.51400000e+03\n",
      "   3.68203734e-01   7.07220574e+00   9.99505440e-01   6.15544832e+00\n",
      "   6.61552918e+01   3.85281691e+01   5.95341677e+00   1.00000000e+00\n",
      "   1.00000000e+00   1.00000000e+00   1.00000000e+00   1.02472799e+00\n",
      "   2.18199802e+00   2.79030663e+00   6.01088032e+00   6.04352127e+00\n",
      "   1.04010880e+01   1.76498516e+01   2.15430267e+01   2.92843719e+01\n",
      "   4.56824342e-01   2.79188922e+01   4.57384599e+02   2.29303561e+03\n",
      "   3.96586281e+05   6.28935450e+02   3.19737883e+01   3.20000000e+01\n",
      "   3.20000000e+01   3.20000000e+01   3.20573689e+01   3.40534125e+01\n",
      "   3.53061325e+01   5.65019782e+01   1.11826904e+03   1.49949011e+03\n",
      "   1.50000000e+03   1.50000000e+03   1.50000000e+03   4.01700000e-01\n",
      "   3.46191889e-03   7.84834797e+01   2.54959941e+02   2.73647779e+03\n",
      "   5.20562522e+01   1.35662710e+01   4.30257171e+01   4.84134520e+01\n",
      "   5.07957468e+01   5.19540059e+01   5.31196835e+01   5.42858556e+01\n",
      "   5.88956479e+01   7.21186944e+01   1.33836795e+02   2.30445598e+02\n",
      "   2.47661227e+02   2.51719585e+02   6.25126434e-01   9.46318759e-01\n",
      "   5.36809278e-02   1.87828684e+03   4.41194857e+03   2.56982641e+03\n",
      "   2.98942280e-01   1.88454500e+02   3.28145049e-01   5.86300692e+00\n",
      "   2.78352760e+04   6.54999896e+04   6.79516142e+08   2.60489362e+04\n",
      "   2.49772502e+01   6.56315529e+01   7.96023739e+01   9.16083086e+01\n",
      "   1.63280910e+02   2.69421860e+02   5.40493076e+02   2.81094496e+04\n",
      "   5.34828022e+04   5.94964580e+04   6.19231350e+04   6.32520371e+04\n",
      "   6.46495297e+04   5.34254060e-01   1.19935707e+01   2.17289742e+04\n",
      "   6.55038299e+04   6.09127003e+08   2.46610722e+04   6.18733927e+01\n",
      "   7.98328388e+01   8.00000000e+01   8.00000000e+01   8.01795252e+01\n",
      "   8.16839763e+01   1.00895153e+02   6.70368744e+03   4.88905500e+04\n",
      "   5.79006439e+04   6.05776098e+04   6.21743704e+04   6.40559822e+04\n",
      "   5.11604253e-01   1.01167161e+01   1.86547972e+00   1.58776775e+01\n",
      "   3.60383284e+02   5.80973913e+01   5.10851053e+00   2.00000000e+00\n",
      "   2.00000000e+00   9.96636993e+00   1.55044510e+01   1.57863501e+01\n",
      "   1.59040554e+01   1.59317507e+01   1.59930762e+01   1.60024728e+01\n",
      "   1.77982196e+01   2.16829871e+01   2.39139466e+01   2.40000000e+01\n",
      "   2.45251484e-01   1.01911485e+00   1.24165139e-04   8.76006746e-03\n",
      "   1.42591199e-04   3.64501217e-06   7.73711667e-02   6.39055287e-02\n",
      "   2.50999748e-06   0.00000000e+00   6.40735391e+02   1.46000000e+03\n",
      "   4.71223897e+05   6.86316291e+02   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.71117705e-01\n",
      "   4.69634026e+00   2.89899110e+02   1.42933729e+03   1.45496142e+03\n",
      "   1.45908605e+03   1.45991691e+03   1.46000000e+03   3.08733264e-01\n",
      "   0.00000000e+00   2.19822195e+04   1.03056878e+07   2.57190203e+10\n",
      "   1.37349333e+05   2.17082097e+01   3.14925816e+01   5.78125618e+01\n",
      "   1.10006924e+02   2.08097428e+02   3.52450544e+02   5.70721563e+02\n",
      "   4.12089318e+03   1.98784886e+04   6.26266192e+04   7.59927873e+04\n",
      "   9.20711939e+04   2.19946567e+05   6.60896048e-01   1.51165776e+03\n",
      "   7.61973788e+02   2.34434313e+04   1.93356304e-01   2.03055999e+04\n",
      "   2.09960419e-01   1.24357072e+01   2.71465129e+04   6.55122735e+04\n",
      "   5.28627720e+08   2.29142591e+04   3.93244313e+01   5.26211672e+01\n",
      "   8.94223541e+01   4.58939169e+02   1.09834421e+03   2.32663699e+03\n",
      "   4.32814639e+03   2.45515544e+04   4.88670396e+04   5.80271340e+04\n",
      "   6.13427661e+04   6.26064006e+04   6.41397250e+04   6.41266691e-01\n",
      "   1.51018793e+01   2.83857521e+04   6.54216296e+04   5.67163096e+08\n",
      "   2.37902938e+04   5.03283877e+01   5.29831850e+01   5.55845697e+01\n",
      "   1.22155292e+02   3.79585559e+02   1.19647923e+03   2.80199555e+03\n",
      "   2.82837779e+04   5.14786207e+04   5.77023225e+04   6.17327161e+04\n",
      "   6.35968506e+04   6.44846647e+04   6.18297008e-01   9.83843620e+03\n",
      "   2.20526553e+04   3.38858907e+09   1.77983333e-01   2.77569659e+09\n",
      "   1.89993814e-01   2.54642498e-01   6.12408501e-01   8.21605781e-02\n",
      "   1.15764936e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[2.7612E7,77582.0...|         1|\n",
      "|[2.9189E7,78524.0...|         1|\n",
      "|[2.9621E7,75075.0...|         1|\n",
      "|[3.0465E7,76962.0...|         1|\n",
      "|[3.0801E7,69624.0...|         1|\n",
      "|[3.0902E7,76576.0...|         1|\n",
      "|[3.1039E7,76399.0...|         1|\n",
      "|[3.1099E7,81844.0...|         1|\n",
      "|[3.1692E7,76181.0...|         1|\n",
      "|[3.2057E7,82251.0...|         1|\n",
      "|[3.2448E7,73984.0...|         1|\n",
      "|[3.2525E7,77413.0...|         1|\n",
      "|[3.2741E7,81946.0...|         1|\n",
      "|[3.2795E7,77418.0...|         1|\n",
      "|[3.2828E7,78132.0...|         1|\n",
      "|[3.2981E7,82120.0...|         1|\n",
      "|[3.3035E7,80597.0...|         1|\n",
      "|[3.3174E7,80055.0...|         1|\n",
      "|[3.3185E7,79492.0...|         1|\n",
      "|[3.3414E7,82803.0...|         1|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "## KMEANS BATCH ##\n",
    "##################\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "(trainingData, testData) = spark_df_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "kmeans = KMeans(k=2, seed=2, initMode=\"random\")\n",
    "model = kmeans.fit(trainingData.select(\"features\"))\n",
    "\n",
    "predictions = model.transform(testData.select(\"features\"))\n",
    "\n",
    "wssse = model.computeCost(trainingData.select(\"features\"))\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "\n",
    "predictions.select(\"features\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-08-07 15:19:34\n",
      "-------------------------------------------\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-07 15:19:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-07 15:19:36\n",
      "-------------------------------------------\n",
      "\n",
      "Final centers: [[  1.76405235e+00   4.00157208e-01   9.78737984e-01   2.24089320e+00\n",
      "    1.86755799e+00  -9.77277880e-01   9.50088418e-01  -1.51357208e-01\n",
      "   -1.03218852e-01   4.10598502e-01   1.44043571e-01   1.45427351e+00\n",
      "    7.61037725e-01   1.21675016e-01   4.43863233e-01   3.33674327e-01\n",
      "    1.49407907e+00  -2.05158264e-01   3.13067702e-01  -8.54095739e-01\n",
      "   -2.55298982e+00   6.53618595e-01   8.64436199e-01  -7.42165020e-01\n",
      "    2.26975462e+00  -1.45436567e+00   4.57585173e-02  -1.87183850e-01\n",
      "    1.53277921e+00   1.46935877e+00   1.54947426e-01   3.78162520e-01\n",
      "   -8.87785748e-01  -1.98079647e+00  -3.47912149e-01   1.56348969e-01\n",
      "    1.23029068e+00   1.20237985e+00  -3.87326817e-01  -3.02302751e-01\n",
      "   -1.04855297e+00  -1.42001794e+00  -1.70627019e+00   1.95077540e+00\n",
      "   -5.09652182e-01  -4.38074302e-01  -1.25279536e+00   7.77490356e-01\n",
      "   -1.61389785e+00  -2.12740280e-01  -8.95466561e-01   3.86902498e-01\n",
      "   -5.10805138e-01  -1.18063218e+00  -2.81822283e-02   4.28331871e-01\n",
      "    6.65172224e-02   3.02471898e-01  -6.34322094e-01  -3.62741166e-01\n",
      "   -6.72460448e-01  -3.59553162e-01  -8.13146282e-01  -1.72628260e+00\n",
      "    1.77426142e-01  -4.01780936e-01  -1.63019835e+00   4.62782256e-01\n",
      "   -9.07298364e-01   5.19453958e-02   7.29090562e-01   1.28982911e-01\n",
      "    1.13940068e+00  -1.23482582e+00   4.02341641e-01  -6.84810091e-01\n",
      "   -8.70797149e-01  -5.78849665e-01  -3.11552532e-01   5.61653422e-02\n",
      "   -1.16514984e+00   9.00826487e-01   4.65662440e-01  -1.53624369e+00\n",
      "    1.48825219e+00   1.89588918e+00   1.17877957e+00  -1.79924836e-01\n",
      "   -1.07075262e+00   1.05445173e+00  -4.03176947e-01   1.22244507e+00\n",
      "    2.08274978e-01   9.76639036e-01   3.56366397e-01   7.06573168e-01\n",
      "    1.05000207e-02   1.78587049e+00   1.26912093e-01   4.01989363e-01\n",
      "    1.88315070e+00  -1.34775906e+00  -1.27048500e+00   9.69396708e-01\n",
      "   -1.17312341e+00   1.94362119e+00  -4.13618981e-01  -7.47454811e-01\n",
      "    1.92294203e+00   1.48051479e+00   1.86755896e+00   9.06044658e-01\n",
      "   -8.61225685e-01   1.91006495e+00  -2.68003371e-01   8.02456396e-01\n",
      "    9.47251968e-01  -1.55010093e-01   6.14079370e-01   9.22206672e-01\n",
      "    3.76425531e-01  -1.09940079e+00   2.98238174e-01   1.32638590e+00\n",
      "   -6.94567860e-01  -1.49634540e-01  -4.35153552e-01   1.84926373e+00\n",
      "    6.72294757e-01   4.07461836e-01  -7.69916074e-01   5.39249191e-01\n",
      "   -6.74332661e-01   3.18305583e-02  -6.35846078e-01   6.76433295e-01\n",
      "    5.76590817e-01  -2.08298756e-01   3.96006713e-01  -1.09306151e+00\n",
      "   -1.49125759e+00   4.39391701e-01   1.66673495e-01   6.35031437e-01\n",
      "    2.38314477e+00   9.44479487e-01  -9.12822225e-01   1.11701629e+00\n",
      "   -1.31590741e+00  -4.61584605e-01  -6.82416053e-02   1.71334272e+00\n",
      "   -7.44754822e-01  -8.26438539e-01  -9.84525244e-02  -6.63478286e-01\n",
      "    1.12663592e+00  -1.07993151e+00  -1.14746865e+00  -4.37820045e-01\n",
      "   -4.98032451e-01   1.92953205e+00   9.49420807e-01   8.75512414e-02\n",
      "   -1.22543552e+00   8.44362976e-01  -1.00021535e+00  -1.54477110e+00\n",
      "    1.18802979e+00   3.16942612e-01   9.20858824e-01   3.18727653e-01\n",
      "    8.56830612e-01  -6.51025593e-01  -1.03424284e+00   6.81594518e-01\n",
      "   -8.03409664e-01  -6.89549778e-01  -4.55532504e-01   1.74791590e-02\n",
      "   -3.53993911e-01  -1.37495129e+00  -6.43618403e-01  -2.22340315e+00\n",
      "    6.25231451e-01  -1.60205766e+00  -1.10438334e+00   5.21650793e-02\n",
      "   -7.39562996e-01   1.54301460e+00  -1.29285691e+00   2.67050869e-01\n",
      "   -3.92828182e-02  -1.16809350e+00   5.23276661e-01  -1.71546331e-01\n",
      "    7.71790551e-01   8.23504154e-01   2.16323595e+00   1.33652795e+00\n",
      "   -3.69181838e-01  -2.39379178e-01   1.09965960e+00   6.55263731e-01\n",
      "    6.40131526e-01  -1.61695604e+00  -2.43261244e-02  -7.38030909e-01\n",
      "    2.79924599e-01  -9.81503896e-02   9.10178908e-01   3.17218215e-01\n",
      "    7.86327962e-01  -4.66419097e-01  -9.44446256e-01  -4.10049693e-01\n",
      "   -1.70204139e-02   3.79151736e-01   2.25930895e+00  -4.22571517e-02\n",
      "   -9.55945000e-01  -3.45981776e-01  -4.63595975e-01   4.81481474e-01\n",
      "   -1.54079701e+00   6.32619942e-02   1.56506538e-01   2.32181036e-01\n",
      "   -5.97316069e-01  -2.37921730e-01  -1.42406091e+00  -4.93319883e-01\n",
      "   -5.42861476e-01   4.16050046e-01  -1.15618243e+00   7.81198102e-01\n",
      "    1.49448454e+00  -2.06998503e+00   4.26258731e-01   6.76908035e-01\n",
      "   -6.37437026e-01  -3.97271814e-01  -1.32880578e-01  -2.97790879e-01\n",
      "   -3.09012969e-01]\n",
      " [  5.51047799e+07   1.05448256e+05   5.99723768e+01   5.14619872e+02\n",
      "    1.51328951e+03   4.10276095e+05   6.39745173e+02   5.99719372e+01\n",
      "    5.99717209e+01   5.99721948e+01   5.99721963e+01   5.99715304e+01\n",
      "    5.99712221e+01   6.00264496e+01   8.16875810e+01   1.30138200e+03\n",
      "    1.51310478e+03   1.51328992e+03   1.51329018e+03   1.51328929e+03\n",
      "    3.68131148e-01   7.09166615e+00   9.99103808e-01   6.13984267e+00\n",
      "    6.57876687e+01   3.74259827e+01   5.86094820e+00   1.00044815e+00\n",
      "    9.99714062e-01   9.99225312e-01   9.99347889e-01   1.04195675e+00\n",
      "    2.30983774e+00   2.91606625e+00   6.00846905e+00   6.03323463e+00\n",
      "    1.02522364e+01   1.76077393e+01   2.10883902e+01   2.88463602e+01\n",
      "    4.52817592e-01   2.79043388e+01   4.57388950e+02   2.18182686e+03\n",
      "    3.96605772e+05   6.28825041e+02   3.19613310e+01   3.19860778e+01\n",
      "    3.19845007e+01   3.19849339e+01   3.20867965e+01   3.42541480e+01\n",
      "    3.54625330e+01   5.58408816e+01   1.12814645e+03   1.49878185e+03\n",
      "    1.49929754e+03   1.49929671e+03   1.49929645e+03   4.01281638e-01\n",
      "    2.59694337e-03   7.83091687e+01   2.54844188e+02   2.71934200e+03\n",
      "    5.18828431e+01   1.44293794e+01   4.28992394e+01   4.84026597e+01\n",
      "    5.07174258e+01   5.18891773e+01   5.30525524e+01   5.42156389e+01\n",
      "    5.88418721e+01   7.16183490e+01   1.33226860e+02   2.29820103e+02\n",
      "    2.47557050e+02   2.51574906e+02   6.24243270e-01   9.44089412e-01\n",
      "    5.47523514e-02   1.93309791e+03   4.48970523e+03   2.48112203e+03\n",
      "    2.96074198e-01   1.82773440e+02   3.25693964e-01   5.62811686e+00\n",
      "    2.79248940e+04   6.54717029e+04   6.80328805e+08   2.60593259e+04\n",
      "    2.48530009e+01   6.55458584e+01   7.95328924e+01   9.13080490e+01\n",
      "    1.60642075e+02   2.52769016e+02   5.05926763e+02   2.88421409e+04\n",
      "    5.34686670e+04   5.95195226e+04   6.19295791e+04   6.32482459e+04\n",
      "    6.46294024e+04   5.35692919e-01   1.17949190e+01   2.17269849e+04\n",
      "    6.54745857e+04   6.08989836e+08   2.46531760e+04   6.02082311e+01\n",
      "    7.97147314e+01   7.99622693e+01   7.99627850e+01   8.03031873e+01\n",
      "    8.14200674e+01   1.02585997e+02   6.62444085e+03   4.89993563e+04\n",
      "    5.78572106e+04   6.05349315e+04   6.21130898e+04   6.40251532e+04\n",
      "    5.11855325e-01   1.01126491e+01   1.86685742e+00   1.58810867e+01\n",
      "    3.57703587e+02   5.76099371e+01   5.10454770e+00   1.99936395e+00\n",
      "    1.99832601e+00   9.99052653e+00   1.55100235e+01   1.57441640e+01\n",
      "    1.58876640e+01   1.59201802e+01   1.59788924e+01   1.59948611e+01\n",
      "    1.77971605e+01   2.18101334e+01   2.39144530e+01   2.39895209e+01\n",
      "    2.46167824e-01   1.02031770e+00   2.00431484e-05   8.70501606e-03\n",
      "   -6.14782765e-04  -1.33140406e-04   7.82099826e-02   6.41646859e-02\n",
      "    5.37860576e-04   6.87566205e-04   6.38959926e+02   1.45931524e+03\n",
      "    4.70605536e+05   6.85701958e+02   1.67038358e-04  -8.29131951e-04\n",
      "    1.66658131e-04   3.81865833e-04   2.76256864e-05   5.00773307e-02\n",
      "    4.07322661e+00   2.93297493e+02   1.42759109e+03   1.45430459e+03\n",
      "    1.45850903e+03   1.45923382e+03   1.45931584e+03   3.09064072e-01\n",
      "    6.66018777e-05   2.26804540e+04   1.20071677e+07   4.15693768e+10\n",
      "    1.62579306e+05   2.20402744e+01   3.12663496e+01   5.69764978e+01\n",
      "    1.06767977e+02   2.03180001e+02   3.47895106e+02   5.67526690e+02\n",
      "    4.10078072e+03   1.99158962e+04   6.23632938e+04   7.53367472e+04\n",
      "    9.07964596e+04   2.23907647e+05   6.62073023e-01   1.51834497e+03\n",
      "    7.78907175e+02   2.22698921e+04   1.88461097e-01   1.94980594e+04\n",
      "    2.03413163e-01   1.23262607e+01   2.68246475e+04   6.54822315e+04\n",
      "    5.25335537e+08   2.28405792e+04   3.94233640e+01   5.25472186e+01\n",
      "    8.68902436e+01   4.34758530e+02   1.07588083e+03   2.24646623e+03\n",
      "    4.18931612e+03   2.38115787e+04   4.85469030e+04   5.78078563e+04\n",
      "    6.12123932e+04   6.25209213e+04   6.40954510e+04   6.40949264e-01\n",
      "    1.51284675e+01   2.83389642e+04   6.53983705e+04   5.64208054e+08\n",
      "    2.37220363e+04   5.04739988e+01   5.29757182e+01   5.55919885e+01\n",
      "    1.22225622e+02   4.48419589e+02   1.34298943e+03   2.97123569e+03\n",
      "    2.79420456e+04   5.14276150e+04   5.77628007e+04   6.17666757e+04\n",
      "    6.35700948e+04   6.44746367e+04   6.22543995e-01   9.89492160e+03\n",
      "    2.22024018e+04   3.39376490e+09   1.75975752e-01   2.77118481e+09\n",
      "    1.89273844e-01   2.51835813e-01   6.15411121e-01   8.15620727e-02\n",
      "    1.11365721e-02]]\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "## KMEANS STREAMING ##\n",
    "######################\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "(trainingData, testData) = spark_df_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "#sc = SparkContext()\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# from https://stackoverflow.com/questions/36142973/how-to-convert-type-row-into-vector-to-feed-to-the-kmeans/43666133#43666133\n",
    "# Turn your dataframe into a dense vector RDD. This step is the one \n",
    "# that had me banging my head against a wall. Prior answers above didn't \n",
    "# spell this out. When you turn your dataframe into an RDD, it is a \n",
    "# bunch of rows with vectors inside them. These need to be converted \n",
    "# into just dense vectors, outside of their row containers. This can be\n",
    "# accomplished like this:\n",
    "\n",
    "trainingDataRDD = trainingData.rdd.map(lambda row: Vectors.dense([x for x in row['features']]))\n",
    "testDataRDD = testData.rdd.map(lambda row: Vectors.dense([x for x in row['features']]))\n",
    "\n",
    "trainingDataRDD = [trainingDataRDD]\n",
    "testDataRDD = [testDataRDD]\n",
    "\n",
    "trainingStream = ssc.queueStream(trainingDataRDD)\n",
    "testingStream = ssc.queueStream(testDataRDD)\n",
    "\n",
    "model = StreamingKMeans(k=2, decayFactor=1.0).setRandomCenters(245, 1.0, 0)\n",
    "\n",
    "model.trainOn(trainingStream)\n",
    "\n",
    "result = model.predictOn(testingStream)\n",
    "result.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "print(\"Final centers: \" + str(model.latestModel().centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time : 0:00:20.396457\n",
      "numNodes =  15\n",
      "depth =  3\n",
      "testing time : 0:00:00.496904\n",
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  1.0|       1.0|[0.16641679160419...|\n",
      "|  1.0|       1.0|[0.16641679160419...|\n",
      "|  0.0|       1.0|[0.37106918238993...|\n",
      "|  0.0|       1.0|[0.37106918238993...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  1.0|       0.0|[0.63333333333333...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  1.0|       1.0|[0.16641679160419...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  1.0|       1.0|[0.37106918238993...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  0.0|       1.0|[0.37106918238993...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  1.0|       0.0|[0.63333333333333...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  0.0|       1.0|[0.37106918238993...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "|  0.0|       0.0|[0.63333333333333...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## ML MODEL PREDICTIONS  ##\n",
    "###########################\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Split te data to training/testing\n",
    "(trainingData, testData) = spark_df_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "start_training = datetime.now()\n",
    "\n",
    "# Train model with Training Data\n",
    "dtModel = dt.fit(trainingData)\n",
    "\n",
    "end_training = datetime.now()\n",
    "print (\"training time : \" + str(end_training - start_training))\n",
    "print \"numNodes = \", dtModel.numNodes\n",
    "print \"depth = \", dtModel.depth\n",
    "\n",
    "start_test = datetime.now()\n",
    "predictions = dtModel.transform(testData)\n",
    "end_test = datetime.now()\n",
    "print (\"testing time : \" + str(end_test - start_test))\n",
    "\n",
    "# Predictions check\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  1.0|       1.0|[0.11358024691358...|\n",
      "|  1.0|       1.0|[0.11358024691358...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  1.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  1.0|       1.0|[0.11358024691358...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  1.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  1.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "|  0.0|       0.0|[0.64330543933054...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "## 10-CV ML MODEL PREDICTIONS ##\n",
    "################################\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "#evaluator.evaluate(predictions)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [1,2])\n",
    "             .build())\n",
    "\n",
    "# Create 10-fold CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "selected.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC : 0.738129713424\n",
      "ACC : 0.741020793951\n",
      "Confustion matrix : \n",
      "DenseMatrix([[ 148.,   56.],\n",
      "             [  81.,  244.]])\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "## EVALUATION METRICS ##\n",
    "########################\n",
    "\n",
    "# Metrics calculation is only included in former RDD API - dataFrame needs to be converted to RDD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "# prediction or probability?\n",
    "predictionAndLabels = predictions.select(\"prediction\",\"label\")\n",
    "\n",
    "metrics_bin = BinaryClassificationMetrics(predictionAndLabels.rdd)\n",
    "metrics_multi = MulticlassMetrics(predictionAndLabels.rdd)\n",
    "\n",
    "# Binary metrics\n",
    "#\t.areaUnderPR - Computes the area under the precision-recall curve.\n",
    "#\t.areaUnderROC - Computes the area under the receiver operating characteristic (ROC) curve.\n",
    "#\t.unpersist() - Unpersists intermediate RDDs used in the computation.\n",
    "print(\"AUC : \" + str(metrics_bin.areaUnderROC))\n",
    "# Multiclass metrics\n",
    "#\t.accuracy - Returns accuracy (equals to the total number of correctly classified instances out of the total number of instances).\n",
    "#\t.confusionMatrix() - Returns confusion matrix: predicted classes are in columns, they are ordered by class label ascending, as in “labels”.\n",
    "#\t.fMeasure(label=None, beta=None) - Returns f-measure or f-measure for a given label (category) if specified.\n",
    "#\t.falsePositiveRate(label) - Returns false positive rate for a given label (category).\n",
    "#\t.precision(label=None) - Returns precision or precision for a given label (category) if specified.\n",
    "#\t.recall(label=None) - Returns recall or recall for a given label (category) if specified.\n",
    "#\t.truePositiveRate(label) - Returns true positive rate for a given label (category).\n",
    "#\t.weightedFMeasure(beta=None) - Returns weighted averaged f-measure.\n",
    "#\t.weightedFalsePositiveRate - Returns weighted false positive rate.\n",
    "#\t.weightedPrecision - Returns weighted averaged precision.\n",
    "#\t.weightedRecall - Returns weighted averaged recall. (equals to precision, recall and f-measure)\n",
    "#\t.weightedTruePositiveRate - Returns weighted true positive rate. (equals to precision, recall and f-measure)\n",
    "print(\"ACC : \" + str(metrics_multi.accuracy))\n",
    "print(\"Confustion matrix : \\n\" + str(metrics_multi.confusionMatrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TBD ###################################\n",
    "##########################################\n",
    "## INITIATE STREAMING CONTEXT for Kafka ##\n",
    "##########################################\n",
    "# according to gudie : https://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html\n",
    "\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-08-07 15:40:42\n",
      "-------------------------------------------\n",
      "(1.0, 0)\n",
      "(2.0, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-08-07 15:40:43\n",
      "-------------------------------------------\n",
      "\n",
      "Final centers: [[ 4.19486462  4.00002246  4.08267685]\n",
      " [ 2.2408932   1.86755799 -0.97727788]]\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "## STREAMING EXAMPLE                         ##\n",
    "## COPIED/ADJUSTED/COMMENTED FROM SOMEWHERE? ##\n",
    "## NOT SURE                                  ##\n",
    "###############################################\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda-4.4.0/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-oracle/jre\"\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/home/big-dama/pavol/moa-release-2018.6.0/lib/moa.jar\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master yarn pyspark-shell\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"StreamingKMeansExample\")  # SparkContext\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    # $example on$\n",
    "    # we make an input stream of vectors for training,\n",
    "    # as well as a stream of vectors for testing\n",
    "    def parse(lp):\n",
    "        label = float(lp[lp.find('(') + 1: lp.find(')')])\n",
    "        vec = Vectors.dense(lp[lp.find('[') + 1: lp.find(']')].split(','))\n",
    "\n",
    "        return LabeledPoint(label, vec)\n",
    "\n",
    "    trainingData = sc.textFile(\"/user/big-dama/pavol/kmeans_data.txt\")\\\n",
    "        .map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "\n",
    "    testingData = sc.textFile(\"/user/big-dama/pavol/streaming_kmeans_data_test.txt\").map(parse)\n",
    "    # 'PipelinedRDD' object has no attribute 'pprint' - testingData.pprint()\n",
    "    # for testing purpose\n",
    "    #print(trainingData.take(2))\n",
    "    #print(testingData.take(2))\n",
    "    \n",
    "    trainingQueue = [trainingData]\n",
    "    testingQueue = [testingData]\n",
    "\n",
    "    trainingStream = ssc.queueStream(trainingQueue)\n",
    "    testingStream = ssc.queueStream(testingQueue)\n",
    "\n",
    "    # We create a model with random clusters and specify the number of clusters to find\n",
    "    model = StreamingKMeans(k=2, decayFactor=1.0).setRandomCenters(3, 1.0, 0)\n",
    "\n",
    "    # Now register the streams for training and testing and start the job,\n",
    "    # printing the predicted cluster assignments on new data points as they arrive.\n",
    "    model.trainOn(trainingStream)\n",
    "\n",
    "    result = model.predictOnValues(testingStream.map(lambda lp: (lp.label, lp.features)))\n",
    "    result.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.stop(stopGraceFully=True)\n",
    "\n",
    "print(\"Final centers: \" + str(model.latestModel().centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "##WORK IN PROGRESS - KIBANA + SPARK ML ##\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to ru this script in terminal to send/stream contents of the mawi dataset to socket -> localhost:12345\n",
    "# if your scripts succesfully listen to socket you will see following output:\n",
    "#big-dama@bigdama-vworker1-phy7:~/pavol/sparkstreaming$ python stream_mawi_csv.py\n",
    "#('\\nListening for a client at', 'localhost', 12345)\n",
    "#('\\nConnected by', ('127.0.0.1', 49952))\n",
    "#\n",
    "#Reading file...\n",
    "#\n",
    "#('Sending line', '36619000,86533,60,423.18,1514,354200,595.14,60,60,60,60,60,60,60,66,807,1514,1514,1514,1514,0.38138,9,1,6.1937,89,26.069,5.1058,1,1,1,1,1,1,1,6,6,17,17,17,17,0.43022,28,370.58,44862,355240,596.02,32,32,32,32,32,32,32,52,201,1500,1500,1500,1500,0.43156,0,79.034,255,2190.7,46.804,1,46,49,51,52,53,54,59,115,120,228,245,251,0.64655,0.95671,0.043269,1258,3318,80,0.40762,80,0.26323,15,23146,65494,706740000,26585,23,80,80,80,80,80,80,873,54447,59995,62324,64103,65013,0.49774,22,27394,65468,673120000,25945,80,80,80,80,80,80,80,21554,53964,59995,64103,64103,64712,0.57312,10,2,15.715,194,16.009,4.0012,2,2,16,16,16,16,16,16,16,16,17,24,24,0.1924,1.0082,0.00014,0.004575,0.00018,0,0.042199,0.056585,0,0,523.86,1460,456810,675.88,0,0,0,0,0,0,0,0,1402,1448,1460,1460,1460,0.26715,0,13897,2097100,1358900000,36863,34,72,129,260,514,725,1031,4172,14174,64436,65450,65535,65535,0.66349,2001,699,62979,0.18476,52342,0.18476,19,28531,65486,667210000,25830,22,53,53,1044,1044,1044,1781,18327,58009,62979,62979,62979,64688,0.57283,19,25635,65476,611230000,24723,53,53,53,123,123,1044,1044,7273,52342,54756,58009,61804,64125,0.5089,9034,20371,3417100000,0.1857,2516700000,0.2287,0.26248,0.57838,0.12935,0.001098,0\\r\\n')\n",
    "\n",
    "%%bash\n",
    "python /home/big-dama/pavol/sparkstreaming/stream_mawi_csv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "done with startup\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda-4.4.0/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-oracle/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[2] --jars /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar --driver-class-path /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar pyspark-shell\"\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setAppName('mawi_dataset_kibana')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"done with startup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## STATIC EXAMPLE THAT WORKS ##\n",
    "###############################\n",
    "##\n",
    "## send the data to Kibana and I am able to find it under \"testindex\" index \n",
    "import json\n",
    "\n",
    "rdd = sc.parallelize([{\"key1\": [\"val1\", \"val2\"]}])\n",
    "final_rdd = rdd.map(json.dumps).map(lambda x: ('key', x))\n",
    "\n",
    "es_write_conf = {\n",
    "    # specify the node that we are sending data to (this should be the master)\n",
    "    \"es.nodes\" : 'localhost',\n",
    "    # specify the port in case it is not the default port\n",
    "    # \"es.port\" : '5601',\n",
    "    # specify a resource in the form 'index/doc-type'\n",
    "    \"es.resource\" : 'testindex/testdoc',\n",
    "    # is the input JSON?\n",
    "    \"es.input.json\" : \"yes\",\n",
    "    # is there a field in the mapping that should be used to specify the ES document ID\n",
    "    #\"es.mapping.id\": \"doc_id\"\n",
    "}\n",
    "\n",
    "final_rdd.saveAsNewAPIHadoopFile(\n",
    "    path='-', \n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "done with startup\n"
     ]
    }
   ],
   "source": [
    "# I tried the same approach as with previous example by use of .foreachRDD(lambda rdd: rdd.saveAsNewAPIHadoopFile(...)), but I couldn't ge it to work\n",
    "# the problem is with converting/writing DStream object to Elastic search, it says it \n",
    "# https://stackoverflow.com/questions/41385293/pyspark-write-dstream-data-to-elasticsearch-using-saveasnewapihadoopfile/50524458\n",
    "# , then I suceeded with following approach:\n",
    "# https://stackoverflow.com/questions/49115508/spark-streaming-write-dataframe-to-elasticsearch\n",
    "# this should be structured streaming approach which is not supported in Cloudera yet? but its working?\n",
    "# https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda-4.4.0/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-oracle/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[2] --jars /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar --driver-class-path /home/big-dama/pavol/sparkstreaming/elasticsearch-hadoop-6.3.2.jar pyspark-shell\"\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"done with startup\")   \n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()\n",
    "\n",
    "# regarding the \"checkpointLocation\" option\n",
    "#    Py4JJavaError: An error occurred while calling o68.start.\n",
    "#: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Could not determine path for the Elasticsearch commit log. Specify the commit log location by setting the [checkpointLocation] option on your DataStreamWriter. If you do not want to persist the Elasticsearch commit log in the regular checkpoint location for your streaming query then you can specify a location to store the log with [es.spark.sql.streaming.sink.log.path], or disable the commit log by setting [es.spark.sql.streaming.sink.log.enabled] to false.\n",
    "        \n",
    "query = lines \\\n",
    ".writeStream \\\n",
    ".outputMode(\"append\") \\\n",
    ".format(\"org.elasticsearch.spark.sql\") \\\n",
    ".option(\"checkpointLocation\", \"/tmp/\") \\\n",
    ".option(\"es.resource\", \"mawitestindex/mawitestdoc\") \\\n",
    ".option(\"es.nodes\", \"localhost\") \\\n",
    ".start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
